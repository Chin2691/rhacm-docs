[#global-hub-configuring]
= Configuring the multicluster global hub

You can configure some settings of the multicluster global hub. 

* <<global-hub-importing-managed-hub-in-default-mode,Importing a managed hub cluster in the default mode (Technology Preview)>> 
* <<global-hub-accessing-grafana-data,Accessing the Grafana data>>
* <<global-hub-grafana-cronjobs-metrics,Setting the cronjobs to gather metrics>>


[global-hub-importing-managed-hub-in-default-mode]
== Importing a managed hub cluster in the default mode (Technology Preview)

To import a managed hub cluster as a managed hub cluster, complete the following steps: 

. Disable the cluster self-management in the existing {product-title} hub cluster by setting the `disableHubSelfManagement` setting to `true` in the `multiclusterhub` custom resource. This setting disables the automatic importing of the hub cluster as a managed cluster.

. Import the managed hub cluster by completing the steps in link:../clusters/cluster_lifecycle/import.adoc#importing-a-target-managed-cluster-to-the-hub-cluster[Importing a target managed cluster to the hub cluster].

. After the managed hub cluster is imported, check the global hub agent status to ensure that the agent is running in the managed hub cluster by running the following command:

----
oc get managedclusteraddon multicluster-global-hub-controller -n ${MANAGED_HUB_CLUSTER_NAME}
----

[global-hub-accessing-grafana-data]
== Accessing the Grafana data

The Grafana data is exposed through the route. Run the following command to display the login URL:

----
oc get route multicluster-global-hub-grafana -n <the-namespace-of-multicluster-global-hub-instance>
----

The authentication method of this URL is same as authenticating to the {ocp} console.

[global-hub-grafana-dashboards]
=== Viewing policy status by using Grafana dashboards

After accessing the global hub Grafana data, you can begin monitoring the policies that were configured through the hub cluster environments that are managed. From the global hub dashboard, you can identify the compliance status of the policies of the system over a selected time range. The policy compliance status is updated daily, so the dashboard does not display the status of the current day until the following day.

To navigate the global hub dashboards, you can observe and filter the policy data by grouping them by `policy` or by `cluster`. If you prefer to examine the policy data by using the `policy` grouping, start from the and the dashboard called `Global Hub - Policy Group Compliancy Overview`. This dashboard allows you to filter the policy data based on `standard`, `category`, and `control`. After selecting a specific point in time on the graph, you are directed to the `Global Hub - Offending Policies` dashboard. The `Global Hub - Offending Policies` dashboard lists the non-compliant or unknown policies at that time. After selecting a target policy, you can view related events and see what has changed by accessing the `Global Hub - What's Changed / Policies` dashboard.

Similarly, if you want to examine the policy data by `cluster` grouping, begin by using the `Global Hub - Cluster Group Compliancy Overview` dashboard. The navigation flow is identical to the `policy` grouping flow, but you select filters that are related to the cluster, such as managed cluster `labels` and `values`. Instead of viewing policy events for all clusters, after reaching the `Global Hub - What's Changed / Clusters` dashboard, you can view policy events related to an individual cluster.

[global-hub-grafana-alerts]
=== Grafana alerts (Technology Preview)

There are three Grafana alerts by and the. These alerts are stored in configmap `multicluster-global-hub-and the-alerting`. They watch suspicious policies, suspicious clusters compliance status change, and failed cron jobs.

* Suspicious policy change - This Alert rule watches the suspicious policies change. If the following events occur more than 5 times in 1 hour, it becomes a firing alert.
+
- A policy was enabled/disabled.
- A policy was updated.

* Suspicious cluster compliance status change - This alert rule watches the cluster compliance status and policy events for a cluster. There are two rules in this alert:
+
- Cluster compliance status change frequently. If a cluster compliance status changes from `compliance` to `non-compliance` more than 3 times in 1 hour, it becomes a firing alert.
- Too many policy events in a cluster. For a policy in a cluster, if there are more than 20 events in 5 minutes, it becomes a firing alert. If this alert is always firing, the data in the `event.local_policies` table increases too fast.

* Cron Job Failed - This alert watch the [Cron jobs](#Cronjobs-and-Metrics) failed events. There are two rules in this alert:
+
- Local compliance job failed. If this alert rule starts firing, it means the local compliance status sync job failed. It might cause the data in the `history.local_compliance` table to be lost. Manually run the job, if necessary.
- Data Retention Job Failed. If this alert rule starts firing, it means the data retention job failed. You can run it manually.

[global-hub-delete-grafana-alert-rule]
==== Delete and the Grafana alert rule

If you find that you do not want all of the and the Grafana alert rules, you can delete a and the Grafana alert rule by including a `deleteRules` section in the `multicluster-global-hub-custom-alerting` configmap. See xref:../global_hub_configuring.adoc#global-hub-customize-grafana-alerting-resources[Customize Grafana alerting resources] for more information about the `multicluster-global-hub-custom-alerting` configmap.

To delete all of the and the alerting, the `deleteRules` configuration section should resemble the following example:

----
    deleteRules:
      - orgId: 1
        uid: globalhub_suspicious_policy_change
      - orgId: 1
        uid: globalhub_cluster_compliance_status_change_frequently
      - orgId: 1
        uid: globalhub_high_number_of_policy_events
      - orgId: 1
        uid: globalhub_data_retention_job
      - orgId: 1
        uid: globalhub_local_compliance_job
----

[global-hub-customize-grafana-alerts]
=== Customize Grafana alerts

Multicluster global hub supports creating custom Grafana alerts. Complete the following steps to customize your Grafana alerts:

[global-hub-customize-grafana-ini-file]
==== Customize your grafana.ini file

To customize your `grafana.ini`` file, create a secret named `multicluster-global-hub-custom-grafana-config` in the `multicluster-global-hub` namespace. The secret data key must be: `grafana.ini`. 
 
The following code shows an example. Replace the required information with your own:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: multicluster-global-hub-custom-grafana-config
  namespace: multicluster-global-hub
type: Opaque
stringData:
  grafana.ini: |
    [smtp]
    enabled = true
    host = smtp.google.com:465
    user = [example@google.com]
    password = [xxx]
    ;cert_file =
    ;key_file =
    skip_verify = true
    from_address = [example@163.com]
    from_name = Grafana 
    # EHLO identity in SMTP dialog (defaults to instance_name)
    ;ehlo_identity = dashboard.example.com
----

Note: you cannot configure the section that already contains the `multicluster-global-hub-and the-grafana-config` secret.

[global-hub-customize-grafana-alerting-resources]
==== Customize Grafana alerting resources

Global hub supports customizing the alerting resources which is explained in link:https://grafana.com/docs/grafana/v10.1/alerting/set-up/provision-alerting-resources/file-provisioning/[Create and manage alerting resources using file provisioning] in the Grafana documentation. Create a configmap named `multicluster-global-hub-custom-alerting` in the `multicluster-global-hub` namespace. The configmap data key must be: `alerting.yaml`. The following code shows an example:

[source,yaml]
----
apiVersion: v1
data:
  alerting.yaml: |
    contactPoints:
      - orgId: 1
        name: globalhub_policy
        receivers:
          - uid: globalhub_policy_alert_email
            type: slack
            type: email
            settings:
              addresses: [example@redhat.com]
              singleEmail: false
          - uid: globalhub_policy_alert_slack
            type: slack
            settings:
              url: [Slack Webhook URL]
              title: |
                {{ template "globalhub.policy.title" . }}
              text: |
                {{ template "globalhub.policy.message" . }}              
    policies:
      - orgId: 1
        receiver: globalhub_policy
        group_by: ['grafana_folder', 'alertname']
        matchers:
          - grafana_folder = Policy
        repeat_interval: 1d
    deleteRules:
      - orgId: 1
        uid: [Alert Rule Uid]
    muteTimes:
      - orgId: 1
        name: mti_1
        time_intervals:
          - times:
              - start_time: '06:00'
                end_time: '23:59'
                location: 'UTC'
            weekdays: ['monday:wednesday', 'saturday', 'sunday']
            months: ['1:3', 'may:august', 'december']
            years: ['2020:2022', '2030']
            days_of_month: ['1:5', '-3:-1']
kind: ConfigMap
metadata:
  name: multicluster-global-hub-custom-alerting
  namespace: multicluster-global-hub
----

[global-hub-grafana-cronjobs-metrics]
=== Setting the cronjobs to gather metrics

After installing the multicluster global hub operand, the global hub manager runs and displays a job scheduler for you to schedule the following cronjobs:

* Local compliance status sync job: This cronjob runs at midnight every day, based on the policy status and events collected by the manager on the previous day. Running this job summarizes the compliance status and the change frequency of the policy on the cluster, and stores them to the `history.local_compliance` table as the data source of the Grafana dashboards. 

* Data retention job: Some data tables in global hub continue to grow over time. Generally, they fall into two categories: the policy event tables and the `history.local_compliance`. Because they grow every day, the tables contain soft deleted records. The policy event tables generate a large amount of data, so range partitioning is used to break down the large tables into small partitions. This partitioning increases the speed of running queries and deletions on these tables. The `history.local_compliance` has a smaller amount of data, and `deletedAt` indexes are added to these tables to obtain better hard delete performance.

At the practical level, a scheduled job runs to delete expired data, which prevents the table from growing too large. There is an additional task that creates a buffer partition table for the next month.

The amount of time that the job should keep the data can be configured through the link:https://github.com/stolostron/multicluster-global-hub/blob/main/operator/apis/v1alpha4/multiclusterglobalhub_types.go#L90[retention] on the global hub operand. It is best practice to use at least a value of one month, and the default value is 18 months. The run interval of this job should be less than one month.  

The listed cronjobs run every time the global hub manager starts. The local compliance status sync job is run once a day and can be run multiple times within the day without changing the result. The data retention job is run once a week and also can be run many times per month without a change in the results. 

The status of these jobs are are saved in the metrics named `multicluster_global_hub_jobs_status`, which can be viewed from the console of the {ocp} cluster. A value of `0` indicates that the job ran successfully, while a value of `1` indicates failure. 

If there is a failed job, you can troubleshoot by using the log tables (`history.local_compliance_job_log`, `event.data_retention_job_log`). See xref:../global_hub/global_hub_trouble_cronjob_compliance_data_restore.adoc#gh-cronjob_compliance_data_restore[Restoring compliance data] for more details and for guidance for deciding whether to run the service manually.
