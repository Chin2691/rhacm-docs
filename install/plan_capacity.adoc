[#sizing-your-cluster]
= Sizing your cluster

Each {product-title} cluster has its own characteristics. There are guidelines that provide sample deployment sizes. Recommendations are classified by size and purpose. The considerations are focused on clusters that are either deployed to VMware or OpenStack environments.

*Note:* The requirements that are listed are not minimum requirements.

[discrete#worker-nodes-workloads]
== Worker nodes (workloads)

As you determine the number of worker nodes and the resource configurations, consider the workload that is running.

* Three worker nodes of approximately equal capacity spread across three availability zones are required.
* If your cluster has a few worker nodes, consider increasing the number of worker nodes while decreasing the size of the nodes for adequate headspace, efficiency, mobility, and resiliency.
* Accommodate the workload mobility.
* Consider the memory that is required for a specific type of workload.
* Consider the memory that is required for other application frameworks.
* The maximum pod per node is 500 and the maximum pod per CPU core is 10.
* The cluster size depends on the worker node number.
The pod number depends on the application type and the worker node's configuration.

A vCPU is equivalent to a Kubernetes compute unit.
For more information, see Kubernetes https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu[Meaning of CPU].

[discrete#red-hat-advanced-cluster-management-for-kubernetes-environment]
== {product-title} environment

* Creating a {ocp} cluster on Amazon Web Services.
+
See the https://docs.openshift.com/container-platform/4.4/installing/installing_aws/installing-aws-customizations.html#installing-aws-customizations[Amazon Web Services information in the {ocp-short} product documentation] for more information.
Also learn more about https://aws.amazon.com/ec2/instance-types/m5/[machine types].

 ** Instance size: m5.xlarge
 ** vCPU: 6
 ** Memory: 16 GB
 ** Storage size: 120 GB

* Creating an {ocp-short} cluster on Google Cloud Platform.
+
See the https://cloud.google.com/docs/quota[Google Cloud Platform product documentation] for more information about quotas.
Also learn more about https://cloud.google.com/compute/docs/machine-types[machine types].

 ** Instance size: N1-standard-4 (0.95--6.5 GB)
 ** vCPU: 6
 ** Memory: 16 GB
 ** Storage size: 120 GB

* Creating an {ocp-short} cluster on Microsoft Azure.
See the following https://docs.openshift.com/container-platform/4.4/installing/installing_azure/installing-azure-account.html[product documentation] for more details.

 ** Instance size: Standard_D2s_v3
 ** vCPU: 6
 ** Memory: 16 GB
 ** Storage size: 120 GB

* Creating an {ocp-short} cluster on VMware vSphere.
See the following https://docs.openshift.com/container-platform/4.5/installing/installing_vsphere/installing-vsphere-installer-provisioned.html[product documentation] for more details.

 ** Cores per socket: 1
 ** CPUs: 2
 ** Memory: 8 GB
 ** Disk size: 120 GB

* Creating an {ocp-short} cluster on bare metal.
See the following https://docs.openshift.com/container-platform/4.4/installing/installing_bare_metal/installing-bare-metal.html[product documentation] for more details.

 ** CPU: 6 (minimum)
 ** Memory: 16 GB (minimum)
 ** Storage size: 120 GB (minimum)
 
[scaling-for-observability]
== Scaling for observability

You need to plan your environment if you want to enable and use the observability service. The resource consumption later is for the {ocp-short} project where observability components are installed. Values that you plan to use are sums for all observability components.

*Note:* Data is based on the results from a lab environment at the time of testing.
Your results might vary, depending on your environment, network speed, and changes to the product.

[sample-observability-environment]
=== Sample observability environment

In the test environment, hub clusters and managed clusters are located in Amazon Web Services cloud platfrom, and have the following topology and configuration. *Note:* Data is based on the results from a lab environment at the time of testing. Your results might vary, depending on your environment, network speed, and changes to the product:

*Master node*

* Instance size: m5.4xlarge
* vCPU: 16
* Memory: 64 GB
* Disk type: gp2
* Disk size: 100 GB
* Count: 3
* Region: sa-east-1

*Worker node*

* Instance size: m5.4xlarge
* vCPU: 16
* Memory: 64 GB
* Disk type: gp2
* Disk size: 100 GB
* Count: 3
* Region: sa-east-1

The observability deployment is configured for high availability environments. With a high availability environment, each Kubernetes deployment has two instances, and each stateful set has three instances.

During the sample test, different number of managed clusters are simulated to push metrics and each test lasts for 24 hours.

*Throughput for each managed cluster*

* Pods: 400
* Interval(minute): 1
* Memory: 64 G

*CPU usage (millicores)*

* 10 clusters: 400
* 20 clusters: 800

*RSS and working set memory*

* 10 clusters: RSS 9.84, working set 4.83
* 20 clusters: RSS 13.10, working set 8.76

+
Memory usage RSS: From the metrics `container_memory_rss` and keeps stability during the test.

Memory usage working set: From the metrics `container_memory_working_set_bytes`, increases along with the test. The following results are from a 24-hour test.

*Persistent volume for `thanos-receive` component* 

*Important:* Metrics are stored in `thanos-receive` until retention time of `thanos-receive` (four days) is reached. 

Other components do not require as much volume as `thanos-receive` components. 

Disk usage increases along with the test. Data represents disk usage after one day, so the final disk usage is mulitplied by four.

* 10 clusters: RSS 9.84, working set 4.83
* 20 clusters: RSS 13.10, working set 8.76

* Network transfer

During tests, network transfer provided stability. See the sizes and network transfer values:

* Size: 10 clusters
* Inbound network transfer: 6.55 MBs per second
* Outbound network transfer: 5.80 MBs per second

* Size: 20 clusters
* Inbound network transfer: 13.08 MBs per second
* Outbound Network Transfer: 10.9 MBs per second

[s3-storage]
=== S3 storage

Total usage in S3 (Amazon Simple Storage Service) increases. The metrics data is stored in S3 until default rentation time (five days) is reached.

* 10 clusters: Total usage 16.2 GB
* 20 clusters: Total usage 23.8 GB
