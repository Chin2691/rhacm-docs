[#performance-and-scalability]
= Performance and scalability

{product-title} is tested to determine certain scalability and performance data.
The major areas that are tested are cluster scalability and search performance.

You can use this information to help you plan your environment.

*Note:* Data is based on the results from a lab environment at the time of testing.
Your results might vary, depending on your environment, network speed, and changes to the product.

[#maximum-number-of-managed-clusters]
== Maximum number of managed clusters

The {product-title} hub cluster provided good performance when managing up to 250 managed clusters.
The following table shows the configuration information for the clusters on the Amazon Web Services cloud platfrom that were used to determine the cluster maximums:

|===
| Node | Flavor | vCPU | RAM (GiB) | Disk type | Disk size (GiB)/IOS | Count | Region

| Master
| m5.2xlarge
| 8
| 32
| gp2
| 100
| 3
| us-east-1

| Worker
| m5.2xlarge
| 8
| 32
| gp2
| 100
| 3/5
| us-east-1
|===

[#search-scalability]
== Search scalability

The scalability of the Search component depends on the performance of the data store.
The following variables are important when analyzing the search performance:

* Physical memory
* Write throughput (Cache recovery time)
* Query execution time

[#physical-memory]
=== Physical memory

Search keeps the data in-memory to achieve fast response times.
The memory required is proportional to the number of Kubernetes resources and their relationships in the cluster.

|===
| Clusters | Kubernetes resources | Relationships | Observed size (with simulated data)

| 1 medium
| 5000
| 9500
| 50 MB

| 5 medium
| 25,000
| 75,000
| 120 MB

| 15 medium
| 75,000
| 20,0000
| 263 MB

| 30 medium
| 150,000
| 450,000
| 492 MB

| 50 medium
| 250,000
| 750,000
| 878 MB
|===

By default, the datastore is deployed with a memory limit of 1 GB.
If you are managing larger clusters, you might need to increase this limit by editing the deployment named `search-prod-xxxxx-redisgraph` in the hub cluster namespace.

[#write-throughput-cache-recovery-time]
=== Write throughput (cache recovery time)

Most clusters in steady state generate a small number of resource updates.
The highest rate of updates happen when the data in RedisGraph is cleared, which causes the remote collectors to synchronize their full state around the same time.

|===
| Clusters | Kubernetes resources | Relationships | Average recovery time from simulation

| 1 medium
| 5000
| 9500
| less than 2 seconds

| 5 medium
| 25,000
| 75,000
| less than 15 seconds

| 15 medium
| 75,000
| 200,000
| 2 minutes and 40 seconds

| 30 medium
| 150,000
| 450,000
| 5-8 minutes
|===

*Remember:* Times might increase for clusters that have a slow network connection to the hub.

[#query-execution-considerations]
=== Query execution considerations

There are some things that can affect the time that it takes to run and return results from a query.
Consider the following items when planning and configuring your environment:

* Searching for a keyword is not efficient.
* The first search takes longer than later searches because it takes additional time to gather the user's access rules.
* The length of time to complete a request is proportional to the number of namespaces and resources the user is authorized to access.
* The worst performance is observed for a request by a non-administrator user with access to all of the namespaces, or all of the managed clusters.

[scaling-for-observability]
== Scaling for observability

You need to plan your environment if you want to enable and use the observability service. The resource consumption later is for the {ocp-short} project where observability components are installed. Values that you plan to use are sums for all observability components.

*Note:* Data is based on the results from a lab environment at the time of testing.
Your results might vary, depending on your environment, network speed, and changes to the product.

[sample-observability-environment]
=== Sample observability environment

In the test environment, hub clusters and managed clusters are located in Amazon Web Services cloud platfrom, and have the following topology and configuration. *Note:* Data is based on the results from a lab environment at the time of testing. Your results might vary, depending on your environment, network speed, and changes to the product:

*Master node*

* Instance size: m5.4xlarge
* vCPU: 16
* Memory: 64 GB
* Disk type: gp2
* Disk size: 100 GB
* Count: 3
* Region: sa-east-1

*Worker node*

* Instance size: m5.4xlarge
* vCPU: 16
* Memory: 64 GB
* Disk type: gp2
* Disk size: 100 GB
* Count: 3
* Region: sa-east-1

The observability deployment is configured for high availability environments. With a high availability environment, each Kubernetes deployment has two instances, and each stateful set has three instances.

During the sample test, different number of managed clusters are simulated to push metrics and each test lasts for 24 hours.

*Throughput for each managed cluster*

* Pods: 400
* Interval(minute): 1
* Memory: 64 G

*CPU usage (millicores)*

* 10 clusters: 400
* 20 clusters: 800

*RSS and working set memory*

* 10 clusters: RSS 9.84, working set 4.83
* 20 clusters: RSS 13.10, working set 8.76

+
Memory usage RSS: From the metrics `container_memory_rss` and keeps stability during the test.

Memory usage working set: From the metrics `container_memory_working_set_bytes`, increases along with the test. The following results are from a 24-hour test.

*Persistent volume for `thanos-receive` component* 

*Important:* Metrics are stored in `thanos-receive` until retention time of `thanos-receive` (four days) is reached. 

Other components do not require as much volume as `thanos-receive` components. 

Disk usage increases along with the test. Data represents disk usage after one day, so the final disk usage is mulitplied by four.

* 10 clusters: RSS 9.84, working set 4.83
* 20 clusters: RSS 13.10, working set 8.76

* Network transfer

During tests, network transfer provided stability. See the sizes and network transfer values:

* Size: 10 clusters
* Inbound network transfer: 6.55 MBs per second
* Outbound network transfer: 5.80 MBs per second

* Size: 20 clusters
* Inbound network transfer: 13.08 MBs per second
* Outbound Network Transfer: 10.9 MBs per second

[s3-storage]
=== S3 storage

Total usage in S3 (Amazon Simple Storage Service) increases. The metrics data is stored in S3 until default rentation time (five days) is reached.

* 10 clusters: Total usage 16.2 GB
* 20 clusters: Total usage 23.8 GB

