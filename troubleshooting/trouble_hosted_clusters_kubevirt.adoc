[#troubleshooting-hosted-clusters-kubevirt]
= Troubleshooting hosted clusters on {ocp-virt}

When you troubleshoot a hosted cluster on {ocp-virt}, start with the top-level `HostedCluster` and `NodePool` resources and then work down the stack until you find the root cause. The following steps can help you discover the root cause of common issues.
 
[#symptom-hosted-cluster-partial]
== Symptom: HostedCluster resource stuck in partial state

A hosted control plane is not coming fully online because a `HostedCluster` resource is pending.

[#identifying-hosted-cluster-partial]
== Identifying the problem: Check prerequisites, resource conditions, and node and operator status

* Ensure that you meet all of the prerequisites for a hosted cluster on {ocp-virt}

* View the conditions on the `HostedCluster` and `NodePool` resources for validation errors that prevent progress.

* By using the `kubeconfig` file of the hosted cluster, inspect the status of the hosted cluster: 

** View the output of the `oc get clusteroperators` command to see which cluster operators are pending. 

** View the output of the `oc get nodes` command to ensure that worker nodes are ready.

[#symptom-hosted-control-plane-no-worker-nodes]
== Symptom: No worker nodes are registered

A hosted control plane is not coming fully online because the hosted control plane has no worker nodes registered.

[#identifying-hosted-control-plane-no-worker-nodes]
== Identifying the problem: Check the status of various parts of the hosted control plane

* View the `HostedCluster` and `NodePool` conditions for failures that indicate what the problem might be.

* Enter the following command to view the KubeVirt worker node virtual machine (VM) status for the `NodePool` resource:

+
----
oc get vm -n <namespace>
----

* If the VMs are stuck in the provisioning state, enter the following command to view the CDI import pods within the VM namespace for clues about why the importer pods have not completed:

+
----
oc get pods -n <namespace> | grep "import"
----

* If the VMs are stuck in the starting state, enter the following command to view the status of the virt-launcher pods:

+
----
oc get pods -n <namespace> -l kubevirt.io=virt-launcher
----

+
If the virt-launcher pods are in a pending state, investigate why the pods are not being scheduled. For example, not enough resources might exist to run the virt-launcher pods.

* If the VMs are running but they are not registered as worker nodes, use the web console to gain VNC access to one of the affected VMs. The VNC output indicates whether the ignition configuration was applied. If a VM cannot access the hosted control plane ignition server on startup, the VM cannot be provisioned correctly.

* If the ignition configuration was applied but the VM is still not registering as a node, go to the <<identifying-vm-console-logs,Identifying the problem: Access the VM console logs>> section to learn how to access the VM console logs during startup.

[#symptom-worker-nodes-stuck]
== Symptom: Worker nodes are stuck in the NotReady state

During cluster creation, nodes enter the `NotReady` state temporarily while the networking stack is rolled out. This part of the process is normal. However, if this part of the process takes longer than 15 minutes, an issue might have occurred.

[#identifying-worker-nodes-stuck]
== Identifying the problem: Investigate the node object and pods

* Enter the following command to view the conditions on the node object and determine why the node is not ready:

+
----
oc get nodes -o yaml
----

* Enter the following command to look for failing pods within the cluster:

+
----
oc get pods -A --field-selector=status.phase!=Running,status,phase!=Succeeded
----

[#symptom-ingress-console-operators-not-online]
== Symptom: Ingress and console cluster operators are not coming online

A hosted control plane is not coming fully online because the Ingress and console cluster operators are not online.

[#identifying-ingress-console-operators-not-online]
== Identifying the problem: Check wildcard DNS routes and load balancer

* If the cluster uses the default Ingress behavior, enter the following command to ensure that wildcard DNS routes are enabled on the {ocp-short} cluster that the VMs are hosted on:

+
----
oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'
----

* If you use a custom base domain for the hosted control plane, complete the following steps:

** Ensure that the load balancer is targeting the VM pods correctly.

** Ensure that the wildcard DNS entry is targeting the load balancer IP.

[#symptom-hosted-cluster-load-balancer]
== Symptom: Load balancer services for the hosted cluster are not available

A hosted control plane is not coming fully online because the load balancer services are not becoming available.

[#identifying-hosted-cluster-load-balancer]
== Identifying the problem: Check events, details, and the kccm pod

* Look for events and details that are associated with the load balancer service within the hosted cluster.

* By default, load balancers for the hosted cluster are handled by the kubevirt-cloud-controller-manager within the hosted control plane namespace. Ensure that the kccm pod is online and view its logs for errors or warnings. To identify the kccm pod in the hosted control plane namespace, enter the following command:

+
----
oc get pods -n <hosted-control-plane-namespace> -l app=cloud-controller-manager
----

[#symptom-hosted-cluster-pvcs-not-available]
== Symptom: Hosted cluster PVCs are not available

A hosted control plane is not coming fully online because the persistent volume claims (PVCs) for a hosted cluster are not available.

[#investigating-hosted-cluster-pvcs-not-available]
== Identifying the problem: Check PVC events and details, and component logs

* Look for events and details that are associated with the PVC to understand which errors are occurring.

* If a PVC is failing to attach to a pod, view the logs for the kubevirt-csi-node daemonset component within the hosted cluster to further investigate the problem. To identify the kubevirt-csi-node pods for each node, enter the following command:

+
----
oc get pods -n openshift-cluster-csi-drivers -o wide -l app=kubevirt-csi-driver
----

* If a PVC cannot bind to a persistent volume (PV), view the logs of the kubevirt-csi-controller component within the hosted control plane namespace. To identify the kubevirt-csi-controller pod within the hosted control plane namespace, enter the following command:

+
----
oc get pods -n <hcp namespace> -l app=kubevirt-csi-driver
----

[#symptom-vm-console-logs]
== Symptom: VM nodes are not correctly joining the cluster

A hosted control plane is not coming fully online because the VM nodes are not correctly joining the cluster.

[#identifying-vm-console-logs]
== Identifying the problem: Access the VM console logs

To access the VM console logs, follow the steps in link:https://access.redhat.com/solutions/7037705[How to get serial console logs for VMs part of OpenShift Virtualization Hosted Control Plane clusters].

//lahinson - oct. 2023 - adding commented-out text to use in the OCP 4.15 timeframe:
// In {ocp-virt} 4.15, enter the following command to access the VM console logs: `oc logs -n <namespace> <vmi_pod> -c guest-console-log`