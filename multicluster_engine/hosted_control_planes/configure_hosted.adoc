[#hosted-control-planes-configure]
= Configuring hosted control planes (Technology Preview)

Configuring hosted control planes requires a hosting service cluster and a hosted cluster. By deploying the HyperShift operator on an existing cluster, you can make that cluster into a hosting service cluster and start the creation of the hosted cluster. 

Hosted control planes is a Technology Preview feature, so the related components are disabled by default. See the following topics to learn more about how to enable hosted control planes.

[#hosting-service-cluster-configure]
== Configuring the hosting service cluster

You can deploy hosted control planes by configuring an existing cluster to function as a hosting service cluster. The hosting service cluster is the {ocp} cluster where the control planes are hosted, and can be the hub cluster or one of the {ocp-short} managed clusters. Complete the following steps on the cluster where the {mce} is installed to enable an {ocp-short} managed cluster as a hosting service cluster:

*Best practice:* Run hosted control planes and worker nodes on the same environment.

[#hosting-service-cluster-configure-prereq]
=== Prerequisites

You must have the following prerequisites to configure a hosting service cluster: 

* {mce} 2.2 and later installed on an {ocp-short} cluster. The {mce} is automatically installed when you install {product-title-short}. {mce} can also be installed without {product-title-short} as an operator from the {ocp-short} OperatorHub.

* The HyperShift binary as a plugin for `oc` to create and manage the hosted cluster in the {mce}. Use one of the following methods to get the binary:
** Navigate to your {ocp-short} console command line tools page. Select the *Hosted Control Plane CLI* tool and follow the instructions to set up the plugin.
** See https://github.com/stolostron/hypershift-addon-operator/blob/main/docs/installing_hypershift_cli.md[Getting started with Hypershift Hosted Control Plane CLI] and follow the instructions.
** Clone and build the binary from the https://github.com/openshift/hypershift[HyperShift repository].

* The {mce} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:
+
----
oc get managedclusters local-cluster
----

[#hosted-create-aws-secret]
=== Creating the Amazon Web Services S3 pull secret

If you plan to create and manage hosted clusters on AWS, complete the following steps:

. Create an OIDC S3 credentials secret named `hypershift-operator-oidc-provider-s3-credentials` for the HyperShift operator.

. Save the secret in the managed cluster namespace, which is the namespace of the managed cluster that is used as the hosting service cluster. If you used `local-cluster`, then create the secret in the `local-cluster` namespace.

. See the following table to make sure that the secret contains the following fields:
+
|===
| Field name | Description

| `bucket`
| Contains an S3 bucket with public access to host OIDC discovery documents for your HyperShift clusters.

| `credentials`
| A reference to a file that contains the credentials of the `default` profile that can access the bucket. By default, HyperShift only uses the `default` profile to operate the `bucket`. 

| `region`
| Specifies the region of the S3 bucket.
|===
+
See https://hypershift-docs.netlify.app/getting-started/[Getting started] in the HyperShift documentation for more information about the secret. The following example shows a sample AWS secret template:
+
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=<s3-bucket-for-hypershift> 
--from-literal=region=<region> -n <hypershift-hosting-service-cluster>
----
+
Replace `hypershift-hosting-service-cluster` with the name of your hosting service cluster.
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-oidc-provider-s3-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-oidc-provider-s3-credentials -n <hypershift-hosting-service-cluster> cluster.open-cluster-management.io/backup=true
----
+
Replace `hypershift-hosting-service-cluster` with the name of your hosting service cluster.

[#hosted-enable-ext-dns]
=== Enabling external DNS

If you plan to provision hosted control plane clusters on the AWS platform with Private Link, complete the following steps:

. Create an AWS credential secret for the HyperShift operator and name it `hypershift-operator-private-link-credentials`. The secret must reside in the managed cluster namespace that is the namespace of the managed cluster being used as the hosting service cluster. If you used `local-cluster`, create the secret in the `local-cluster` namespace.
+
See steps 1-5 in https://hypershift-docs.netlify.app/how-to/aws/deploy-aws-private-clusters/[Deploy AWS private clusters] for more details. 

. Make sure the secret contains the following three fields:
+
See the following table to make sure that the secret contains the required fields:
+
|===
| Field name | Description | Optional or required

| `provider`
| The DNS provider that manages the service-level DNS zone.
| Required

| `domain-filter`
| The service-level domain.
| Required

| `credentials`
| The credential file that supports all external DNS types.
| Optional when using AWS keys

| `aws-access-key-id`
| The credential access key id.
| Optional when using the AWS DNS service and 

| `aws-secret-access-key`
| The credential access key secret.
| Optional when using the AWS DNS service
|===
+
See https://hypershift-docs.netlify.app/getting-started/[Getting started] in the HyperShift documentation for more information about the secret. The following example shows the sample `hypershift-operator-private-link-credentials` secret template:
+
----
oc create secret generic hypershift-operator-private-link-credentials --from-literal=aws-access-key-id=<aws-access-key-id> --from-literal=aws-secret-access-key=<aws-secret-access-key> --from-literal=region=<region> -n <hypershift-hosting-service-cluster>
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-private-link-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-private-link-credentials -n <hypershift-hosting-service-cluster> cluster.open-cluster-management.io/backup=""
----

[#hosted-enable-feature]
=== Enabling the hosted control planes feature

The hosted control planes features is disabled by default. Enabling the feature automatically enables the HyperShift add-on as well. The following example shows the `spec.overrides` default template used to enable the hosted control planes feature:
+
*Important:* You must create the AWS S3 pull secret before enabling the hosted control planes feature.

[source,yaml]
----
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: <namespace>
spec:
  overrides:
    components:
    - name: hypershift-preview
      enabled: true
----
+
Replace `namespace` with the name of your project.

You can run the following command to check if the hosted control planes feature is enabled:

----
oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'
----

[#hosted-enable-hypershift-add-on]
==== Manually enabling the HyperShift add-on

Enabling the hosted control planes features automatically enables the HyperShift add-on. If you need to enable the HyperSHift add-on manually, complete the following steps to use the `hypershift-addon` to install the HyperShift operator on a managed cluster:
+
*Note:* The `local-cluster` on the {mce} hub cluster is set as the hosting service cluster by default. If you are using the `local cluster`, continue to step 5.
+
.. Create a namespace where the HyperShift operator is created. 

.. Create the `ManagedClusterAddon` HyperShift add-on by creating a file that resembles the following example:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: <managed-cluster-name> 
spec:
  installNamespace: open-cluster-management-agent-addon
----
+
Replace `managed-cluster-name` with the name of the managed cluster on which you want to install the HyperShift operator.

.. Apply the file by running the following command:
+
----
oc apply -f <filename>
----
+
Replace `filename` with the name of the file that you created. 

. Confirm that the `hypershift-addon` is installed by running the following command:
+
----
oc get managedclusteraddons -n <hypershift-hosting-service-cluster> hypershift-addon
----
+
If the add-on is installed, the output resembles the following example:
+
----
NAME               AVAILABLE   DEGRADED   PROGRESSING
hypershift-addon   True
----

Your HyperShift add-on is installed and the hosting service cluster is available to manage HyperShift clusters.

[#hosted-deploy-cluster-aws]
=== Deploying a hosted cluster on AWS

After setting up the HyperShift binary and enabling your chosen cluster as the hosting service cluster, you can deploy a hosted cluster on AWS by completing the following steps:

. Set environment variables as follows, replacing variables as needed with your credentials:
+
----
export REGION=us-east-1
export CLUSTER_NAME=clc-name-hs1
export INFRA_ID=clc-name-hs1
export BASE_DOMAIN=dev09.red-chesterfield.com
export AWS_CREDS=$HOME/name-aws
export PULL_SECRET=/Users/username/pull-secret.txt
export BUCKET_NAME=acmqe-hypershift
export BUCKET_REGION=us-east-1
----
+
*Best practice:* Make sure `CLUSTER_NAME` and `INFRA_ID` have the same values, otherwise the cluster might not appear correctly in the {mce} console.

. Make sure you are logged into your hub cluster.

. Run the following command to create the hosted cluster:
+
----
oc hcp create cluster aws \
    --name $CLUSTER_NAME \
    --infra-id $INFRA_ID \
    --aws-creds $AWS_CREDS \
    --pull-secret $PULL_SECRET \
    --region $REGION \
    --generate-ssh \
    --node-pool-replicas 3 \
    --namespace <hypershift-hosting-service-cluster>
----

. You can check the status of your hosted cluster by running the following command:
+
----
oc get hostedclusters -n <hypershift-hosting-service-cluster>
----

[#deploying-sr-iov]
=== Deploying the SR-IOV Operator

After you configure and deploy the hosting service cluster, you can create a subscription to the SR-IOV Operator on a hosted cluster. The SR-IOV pod runs on workers instead of control planes.

. Create a namespace and an operator group:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
----

. Create a subscription to the SR-IOV Operator: 
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subsription
  namespace: openshift-sriov-network-operator
spec:
  channel: "4.12"
  name: sriov-network-operator
  config:
    nodeSelector:
      node-role.kubernetes.io/worker: ""
  source: s/qe-app-registry/redhat-operators
  sourceNamespace: openshift-marketplace
----

. To verify that the SR-IOV Operator is ready, run the following command and view the resulting output:
+
----
oc get csv -n openshift-sriov-network-operator
----
+
----
NAME                                         DISPLAY                   VERSION               REPLACES                                     PHASE
sriov-network-operator.4.12.0-202211021237   SR-IOV Network Operator   4.12.0-202211021237   sriov-network-operator.4.12.0-202210290517   Succeeded
----

. To verify that the SR-IOV pods are deployed, run the following command:
+
----
oc get pods -n openshift-sriov-network-operator
----