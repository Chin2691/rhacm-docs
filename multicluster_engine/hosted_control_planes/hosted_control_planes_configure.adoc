[#hosted-control-planes-configure]
= Configuring hosted control planes

Configuring hosted control planes requires a hosting service cluster and a hosted cluster. By deploying the HyperShift operator on an existing cluster, you can make that cluster into a hosting service cluster and start the creation of the hosted cluster. 

Hosted control planes is a Technology Preview feature, so the related components are disabled by default. Run the following command to check if the feature is enabled:

----
oc patch mce <mce-instance-name> --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'
----

*Note:* Turning on the hosted control planes feature automatically installs the HyperShift add-on.

See the following topics to learn more about how to enable hosted control planes.

[#hosting-service-cluster-configure]
== Configuring the hosting service cluster

You can deploy hosted control planes by configuring an existing cluster to function as a hosting service cluster. The hosting service cluster is the {ocp} cluster where the control planes are hosted, and can be the hub cluster or one of the {ocp-short} managed clusters.

*Best practice:* Run hosted control planes and worker nodes on the same environment.

[#hosting-service-cluster-configure-prereq]
=== Prerequisites

You must have the following prerequisites to configure a hosting service cluster: 

* {mce} 2.2 and later installed on an {ocp-short} cluster. The {mce} is automatically installed when you install {product-title-short}. {mce} can also be installed without {product-title-short} as an operator from the {ocp-short} OperatorHub.

* The HyperShift binary as a plugin to `oc` to create and amanage the hosted cluster in the {mce}. Use one of the following methods to get the binary:
** Navigate to your {ocp-short} console command line tools page. Select the *Hosted Control Plane CLI* tool and follow the instructions to set up `oc` plugin.
** See https://github.com/stolostron/hypershift-addon-operator/blob/main/docs/installing_hypershift_cli.md[Getting started with Hypershift Hosted Control Plane CLI] and follow the instructions.
** Clone and build the binary from the https://github.com/openshift/hypershift[HyperShift repository].

* {mce} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster` You can check the status of your hub cluster by running the following command:
+
----
oc get managedclusters local-cluster
----

[#hosting-service-cluster]
=== Configure the hosting service cluster

Complete the following steps on the cluster where the {mce} is installed to enable an {ocp-short} managed cluster as a hosting service cluster:

. If you plan to create and manage hosted clusters on AWS, create an OIDC S3 credentials secret named `hypershift-operator-oidc-provider-s3-credentials` for the HyperShift operator. Save the secret in the managed cluster namespace, which is the namespace of the managed cluster that is used as the hosting service cluster. If you used `local-cluster`, then create the secret in the `local-cluster` namespace.
+
The secret must contain the following three fields:
+
* The `bucket` field contains an S3 bucket with public access to host OIDC discovery documents for your HyperShift clusters.
* The `credentials` field is a reference to a file that contains the credentials of the `default` profile that can access the bucket. By default, HyperShift only uses the `default` profile to operate the `bucket`. 
* The `region` field specifies the region of the S3 bucket.
+
See https://hypershift-docs.netlify.app/getting-started/[Getting started] in the HyperShift documentation for more information about the secret. The following example shows a sample AWS secret template:
+
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=<s3-bucket-for-hypershift> 
--from-literal=region=<region> -n <hypershift-hosting-service-cluster>
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-oidc-provider-s3-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-oidc-provider-s3-credentials -n <hypershift-hosting-service-cluster> cluster.open-cluster-management.io/backup=true
----

. If you plan to provision hosted clusters on the AWS platform with Private Link, create an AWS credential secret for the HyperShift operator and name it `hypershift-operator-private-link-credentials`. It must reside in the managed cluster namespace that is the namespace of the managed cluster being used as the hosting service cluster. If you used `local-cluster`, create the secret in the `local-cluster` namespace. See steps 1-5 in https://hypershift-docs.netlify.app/how-to/aws/deploy-aws-private-clusters/[Deploy AWS private clusters] for more details. 
+
The secret must contain the following three fields:
+
* `aws-access-key-id`: AWS credential access key id
+
* `aws-secret-access-key`: AWS credential access key secret
+
* `region`: Region for use with Private Link
+
See https://hypershift-docs.netlify.app/getting-started/[Getting started] in the HyperShift documentation for more information about the secret. The following example shows the sample `hypershift-operator-private-link-credentials` secret template:
+
----
oc create secret generic hypershift-operator-private-link-credentials --from-literal=aws-access-key-id=<aws-access-key-id> --from-literal=aws-secret-access-key=<aws-secret-access-key> --from-literal=region=<region> -n <hypershift-hosting-service-cluster>
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-private-link-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-private-link-credentials -n <hypershift-hosting-service-cluster> cluster.open-cluster-management.io/backup=""
----
+
Set the following parameter in the `HypershiftDeployment` custom resource when creating a cluster:
+
[source,yaml]
----
spec:
  hostedClusterSpec:
    platform:
      type: AWS
      aws:
        endpointAccess: Private
----
+
. If you plan to use service-level DNS for control plane services on the AWS platform, create an external DNS credential secret for the HyperShift operator and name it `hypershift-operator-external-dns-credentials`. It must reside in the managed cluster namespace that is the namespace of the managed cluster being used as the hosting service cluster. If you used `local-cluster`, then create the secret in the `local-cluster` namespace.
+
The secret must contain the following three fields:
+
* `provider`: DNS provider that manages the service-level DNS zone (example: aws)
+
* `domain-filter`: The service-level domain
+
* `credentials`: (Optional, only when using AWS keys) - For all external DNS types, a credential file is supported
+
* `aws-access-key-id`: (Optional) - When using AWS DNS service, credential access key id
+
* `aws-secret-access-key`: (Optional) - When using AWS DNS service, credential access key secret
+
See https://hypershift-docs.netlify.app/how-to/external-dns/[Use Service-level DNS for Control Plane Services] for more details. The following example shows the sample `hypershift-operator-external-dns-credentials` secret template:
+
----
oc create secret generic hypershift-operator-external-dns-credentials --from-literal=provider=aws --from-literal=domain-filter=service.my.domain.com --from-file=credentials=<credentials-file> -n <hypershift-hosting-service-cluster>
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-external-dns-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-external-dns-credentials -n <hypershift-hosting-service-cluster> cluster.open-cluster-management.io/backup=true
----
+
. Enable the HyperShift add-on.
+
The cluster that hosts the HyperShift operator is the hosting service cluster. This step uses the `hypershift-addon` to install the HyperShift operator on a managed cluster.
+
*Note:* The `local-cluster` on {mce} hub cluster is set as the hosting service cluster by default. If you are using the `local cluster`, continue to step 5.
+
.. Create a namespace where the HyperShift operator is created. 

.. Create the `ManagedClusterAddon` HyperShift add-on by creating a file that resembles the following example:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: <managed-cluster-name> 
spec:
  installNamespace: open-cluster-management-agent-addon
----
+
Replace `managed-cluster-name` with the name of the managed cluster on which you want to install the HyperShift operator.

.. Apply the file by running the following command:
+
----
oc apply -f <filename>
----
+
Replace `filename` with the name of the file that you created. 

. Confirm that the `hypershift-addon` is installed by running the following command:
+
----
oc get managedclusteraddons -n <hypershift-hosting-service-cluster> hypershift-addon
----
+
If the add-on is installed, the output resembles the following example:
+
----
NAME               AVAILABLE   DEGRADED   PROGRESSING
hypershift-addon   True
----

Your HyperShift add-on is installed and the hosting service cluster is available to manage HyperShift clusters.

[#hosted-deploy-cluster-aws]
== Deploying a hosted cluster on AWS

After setting up the HyperShift binary and enabling your chosen cluster as the hosting service cluster, you can deploy a hosted cluster on AWS by completing the following steps:

. Set environment variables that resemble the following example:
+
----
export REGION=us-east-1
export CLUSTER_NAME=clc-name-hs1
export INFRA_ID=clc-name-hs1
export BASE_DOMAIN=dev09.red-chesterfield.com
export AWS_CREDS=$HOME/name-aws
export PULL_SECRET=/Users/username/pull-secret.txt
export BUCKET_NAME=acmqe-hypershift
export BUCKET_REGION=us-east-1
----
+
*Best practice:* Make sure have the same variables, otherwise the cluster might not appear correctly in the {mce} console.

. Make sure you are logged into your hub cluster.

. Run the following command to create the hosted cluster:
+
----
oc hcp create cluster aws \
    --name $CLUSTER_NAME \
    --infra-id $INFRA_ID \
    --aws-creds $AWS_CREDS \
    --pull-secret $PULL_SECRET \
    --region $REGION \
    --generate-ssh \
    --node-pool-replicas 3 \
    --namespace <hypershift-hosting-service-cluster>
----

. You can check the status of your hosted cluster by running the following command:
+
----
oc get hostedclusters -n <hypershift-hosting-service-cluster>
----

See link:../cluster_lifecycle/import_gui.adoc#importing-hosted-cluster-ui[Importing a hosted control plane cluster] or link:../cluster_lifecycle/import_cli.adoc#importing-hosted-cluster-cli[Importing a hosted control plane cluster with the CLI] to learn how to import the hosted cluster.

[#deploying-sr-iov]
== Deploying the SR-IOV Operator

After you configure and deploy the hosting service cluster, you can create a subscription to the SR-IOV Operator on a hosted cluster. The SR-IOV pod runs on workers rather than control planes.

. Create a namespace and an operator group:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
----

. Create a subscription to the SR-IOV Operator: 
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subsription
  namespace: openshift-sriov-network-operator
spec:
  channel: "4.12"
  name: sriov-network-operator
  config:
    nodeSelector:
      node-role.kubernetes.io/worker: ""
  source: s/qe-app-registry/redhat-operators
  sourceNamespace: openshift-marketplace
----

. To verify that the SR-IOV Operator is ready, run the following command and view the resulting output:
+
----
# oc get csv -n openshift-sriov-network-operator
----
+
----
NAME                                         DISPLAY                   VERSION               REPLACES                                     PHASE
sriov-network-operator.4.12.0-202211021237   SR-IOV Network Operator   4.12.0-202211021237   sriov-network-operator.4.12.0-202210290517   Succeeded
----

. To verify that the SR-IOV pods are deployed, run the following command:
+
----
oc get pods -n openshift-sriov-network-operator
----

[#hosting-service-cluster-access]
== Accessing a hosting service cluster

You can now access your cluster. The access secrets are stored in the `hypershift-management-cluster` namespace. Learn about the following formats secret name formats:

- `kubeconfig` secret: `<hostingNamespace>-<name>-admin-kubeconfig` (clusters-hypershift-demo-admin-kubeconfig)
- `kubeadmin` password secret: `<<hostingNamespace>-<name>-kubeadmin-password` (clusters-hypershift-demo-kubeadmin-password)
