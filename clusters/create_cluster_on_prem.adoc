[#creating-a-cluster-on-premises]
= Creating a cluster in an on-premises environment

You can use the {product-title} console to create an on-premises {ocp} cluster. This process is recommended instead of the bare metal process that is deprecated.

It is best practice for you to use this procedure for creating a single-node OpenShift (SNO) cluster. You can create a single-node OpenShift cluster on VMware vSphere, Red Hat OpenStack, Red Hat Virtualization Platform, and in a bare metal environment. A single-node OpenShift cluster contains only a single node which hosts the control plane services and the user workloads. This configuration can be helpful when you want to minimize the resource footprint of the cluster. 

You can also test the procedure of provisioning multiple single-node OpenShift clusters on edge resources by using the zero touch provisioning feature, which is a Technology Preview feature that is available with {ocp}. For more information about that procedure, see https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html/scalability_and_performance/ztp-deploying-disconnected[Deploying distributed units at scale in a disconnected environment] in the {ocp-short} documentation.

* <<on-prem-prerequisites,Prerequisites>>
* <<on-prem-creating-your-cluster-with-the-console,Creating your cluster with the console>>

[#on-prem-prerequisites]
== Prerequisites

See the following prerequisites before creating a cluster in an on-premises environment:

* You must have a deployed {product-title-short} hub cluster on {ocp-short} version 4.9, or later.
* You need a configured infrastructure environment with configured hosts. See xref:../clusters/create_infra_env.adoc#creating-an-infrastructure-environment[Creating an infrastructure environment] for more information.
* You must have Internet access for your {product-title} hub cluster (connected), or a connection to an internal or mirror registry that has a connection to the Internet (disconnected) to retrieve the required images for creating the cluster.
* You need a configured on-premises credential. See link:../credentials/credential_on_prem.adoc#creating-a-credential-for-an-on-premises-environment[Creating a credential for an on-premises environment] for more information.
* You need an {ocp-short} image pull secret. See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/images/managing-images#using-image-pull-secrets[Using image pull secrets] for more information.


[#on-prem-creating-your-cluster-with-the-console]
== Creating your cluster with the console

To create a cluster from the {product-title} console, navigate to *Infrastructure* > *Clusters*. On the _Clusters_ page, click *Create cluster* and complete the steps in the console.

The following options are available for your assisted installation: 

* *Use existing discovered hosts*: Select your hosts from a list of hosts that are in an existing infrastructure environment.

* *Discover new hosts*: Discover hosts that are not already in an existing infrastructure environment. Discover your own hosts, rather than using one that is already in an infrastructure environment.

If you need to create a credential, see link:../credentials/credential_on_prem.adoc#creating-a-credential-for-an-on-premises-environment[Creating a credential for an on-premises environment] for more information.

The name for your cluster is used in the hostname of the cluster.

*Important:* When you create a cluster, the {product-title-short} controller creates a namespace for the cluster and its resources. Ensure that you include only resources for that cluster instance in that namespace. Destroying the cluster deletes the namespace and all of the resources in it.

*Tip:* Select *YAML: On* to view content updates as you enter the information in the console.

If you want to add your cluster to an existing cluster set, you must have the correct permissions on the cluster set to add it. If you do not have `cluster-admin` privileges when you are creating the cluster, you must select a cluster set on which you have `clusterset-admin` permissions. If you do not have the correct permissions on the specified cluster set, the cluster creation fails. Contact your cluster administrator to provide you with `clusterset-admin` permissions to a cluster set if you do not have any cluster set options to select.

Every managed cluster must be associated with a managed cluster set. If you do not assign the managed cluster to a `ManagedClusterSet`, it is automatically added to the `default` managed cluster set.

If there is already a base DNS domain that is associated with the selected credential that you configured for your provider account, that value is populated in that field. You can change the value by overwriting it, but this setting cannot be changed after the cluster is created. The base domain of your provider is used to create routes to your {ocp} cluster components. It is configured in the DNS of your cluster provider as a Start of Authority (SOA) record. 

The *OpenShift version* identifies the version of the {ocp-short} image that is used to create the cluster. If the version that you want to use is available, you can select the image from the list of images. If the image that you want to use is not a standard image, you can enter the URL to the image that you want to use. See xref:../clusters/release_images.adoc#release-images[Release images] for more information about release images. 

When you select an OpenShift version that is 4.9 or later, an option to select *Install single node OpenShift (SNO)* is displayed. A single-node OpenShift cluster contains a single node which hosts the control plane services and the user workloads.

If you want your cluster to be a single-node OpenShift cluster, select the single-node OpenShift option. If you are creating a single-node OpenShift cluster, add the `spec:skipMachinePools` parameter in the YAML content with a value of `true` by completing the following steps:

. If you cannot see the YAML content window, set the switch to *YAML:On* to view the content.

. Find the `spec` section for your cluster and add the following flag to the `spec` section:
+
[source,yaml]
----
skipMachinePools: true
----

*Tip:* After you review and save the cluster, your cluster is saved as a draft cluster. You can close the creation process and finish the process later by selecting the cluster name on the _Clusters_ page.

If you are using existing hosts, select whether you want to select the hosts yourself, or if you want them to be selected automatically. The number of hosts is based on the number of nodes that you selected. For example, a SNO cluster only requires one host, while a standard three-node cluster requires three hosts. 

The locations of the available hosts that meet the requirements for this cluster are displayed in the list of _Host locations_. For distribution of the hosts and a more high-availability configuration, select multiple locations.

If you are discovering new hosts with no existing infrastructure environment, complete the steps in xref:../clusters/add_hosts_infra_env.adoc#adding-hosts-to-an-infrastructure-environment[Adding hosts to an infrastructure environment] beginning with step 4 to define your hosts.   

After the hosts are bound, and the validations pass, complete the networking information for your cluster by adding the following IP addresses: 

* API VIP: Specifies the IP address to use for internal API communication.
+
*Note:* This value must match the name that you used to create the DNS records listed in the prerequisites section. If not provided, the DNS must be pre-configured so that `api.` resolves correctly.

* Ingress VIP: Specifies the IP address to use for ingress traffic.
+
*Note:* This value must match the name that you used to create the DNS records listed in the prerequisites section. If not provided, the DNS must be pre-configured so that `test.apps.` resolves correctly.

You can view the status of the installation on the _Clusters_ navigation page.

Continue with xref:../clusters/access_cluster.adoc#accessing-your-cluster[Accessing your cluster] for instructions for accessing your cluster. 

Your cluster is created. Continue with xref:../clusters/access_cluster.adoc#accessing-your-cluster[Accessing your cluster] for instructions for accessing your cluster. 

[#dynamic-metrics-for-sno]
== Dynamic metrics for single-node OpenShift clusters

Dynamic metrics collection supports automatic metric collection based on certain conditions. By default, a SNO cluster does not collect pod and container resource metrics. Once a SNO cluster reaches a specific level of resource consumption, the defined granular metrics are collected dynamically. When the cluster resource consumption is consistently less than the threshold for a period of time, granular metric collection stops.

The metrics are collected dynamically based on the conditions on the managed cluster specified by a collection rule. Because these metrics are collected dynamically, the following {product-title-short} Grafana dashboards do not display any data. When a collection rule is activated and the corresponding metrics are collected, the following panels display data for the duration of the time that the collection rule is initiated:

* Kubernetes / Compute Resources / Namespace (Pods)
* Kubernetes / Compute Resources / Namespace (Workloads)
* Kubernetes / Compute Resources / Nodes (Pods)
* Kubernetes / Compute Resources / Pod
* Kubernetes / Compute Resources / Workload

A collection rule includes the following conditions:

* A set of metrics to collect dynamically.
* Conditions written as a PromQL expression.
* A time interval for the collection, which must be set to `true`.
* A match expression to select clusters where the collect rule must be evaluated.

By default, collection rules are evaluated continuously on managed clusters every 30 seconds, or at a specific time interval. The lowest value between the collection interval and time interval takes precedence. Once the collection rule condition persists for the duration specified by the `for` attribute, the collection rule starts and the metrics specified by the rule are automatically collected on the managed cluster. Metrics collection stops automatically after the collection rule condition no longer exists on the managed cluster, at least 15 minutes after it starts.

The collection rules are grouped together as a parameter section named `collect_rules`, where it can be enabled or disabled as a group. {product-title-short} installation includes the collection rule group, `SNOResourceUsage` with two default collection rules: `HighCPUUsage` and `HighMemoryUsage`. The `HighCPUUsage` collection rule begins when the node CPU usage exceeds 70%. The `HighMemoryUsage` collection rule begins if the overall memory utilization of the SNO cluster exceeds 70% of the available node memory. Currently, the previously mentioned thresholds are fixed and cannot be changed. When a collection rule begins for more than the interval specified by the `for` attribute, the system automatically starts collecting the metrics that are specified in the `dynamic_metrics` section.

View the list of dynamic metrics that from the `collect_rules` section, in the following YAML file:

[source,yaml]
----
collect_rules:
  - group: SNOResourceUsage
    annotations:
      description: >
        By default, a SNO cluster does not collect pod and container resource metrics. Once a SNO cluster 
        reaches a level of resource consumption, these granular metrics are collected dynamically. 
        When the cluster resource consumption is consistently less than the threshold for a period of time, 
        collection of the granular metrics stops.
    selector:
      matchExpressions:
        - key: clusterType
          operator: In
          values: ["SNO"]
    rules:
    - collect: SNOHighCPUUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster cpu usage is constantly more than 70% for 2 minutes
      expr: (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - container_cpu_cfs_periods_total
          - container_cpu_cfs_throttled_periods_total
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests   
          - namespace_workload_pod:kube_pod_owner:relabel 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate 
    - collect: SNOHighMemoryUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster memory usage is constantly more than 70% for 2 minutes
      expr: (1 - sum(:node_memory_MemAvailable_bytes:sum) / sum(kube_node_status_allocatable{resource=\"memory\"})) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests 
          - namespace_workload_pod:kube_pod_owner:relabel
        matches:
          - __name__="container_memory_cache",container!=""
          - __name__="container_memory_rss",container!=""
          - __name__="container_memory_swap",container!=""
          - __name__="container_memory_working_set_bytes",container!=""
----

A `collect_rules.group` can be disabled in the `custom-allowlist` as shown in the following example. When a `collect_rules.group` is disabled, metrics collection reverts to the previous behavior. These metrics are collected at regularly, specified intervals:

[source,yaml]
----
collect_rules:
  - group: -SNOResourceUsage
---- 

The data is only displayed in Grafana when the rule is initiated.
