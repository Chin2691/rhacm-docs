[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp} cluster, see https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12[{ocp-short} release notes].

* <<installation-known-issues,Installation known issues>>
* <<cluster-lifecycle-issues,Cluster lifecycle>>
* <<hosted-control-plane-mce,Hosted control plane>>

[#installation-known-issues]
== Installation known issues

[#cluster-lifecycle-issues]
== Cluster lifecycle

[#id-k8s-io-not-updated-311]
=== Value of the cluster claim _id.k8s.io_ on {ocp-short} version 3.11 is lost after upgrading to {product-title-short} 2.7
//2.7:ACM3000

After upgrading to {product-title-short} 2.7, the value of cluster claim `id.k8s.io` is missing for a managed cluster that is running on {ocp-short} 3.11. The value is stored in the `clusterclaims.cluster.open-cluster-management.io` file.

To cause the value to repopulate, manually delete the `id.k8s.io` setting by entering the following command:

----
oc delete clusterclaims.cluster.open-cluster-management.io  id.k8s.io
----

The cluster claim is re-created automatically.

[#discon-disc-iso-cluster-no-install]
=== A cluster that is deployed in a disconnected environment by using the Central Infrastructure Management service might not install
//2.7:ACM3209

When you deploy a cluster in a disconnected environment using the Central Infrastructure Management service, the cluster nodes might not start installing. This issue occurs when the cluster uses a discovery ISO image that is derived from the Red Hat Enterprise Linux CoreOS live ISO image that is shipped with {ocp-short} 4.12.0/4.12.1.

The issue is that the Red Hat Enterprise Linux CoreOS live ISO image that is shipped with {ocp-short} 4.12.0/4.12.1 contains a restrictive `/etc/containers/policy.json` file that requires signatures for images sourcing from `registry.redhat.io` and `registry.access.redhat.com`. In a disconnected environment, the images that are mirrored might not have the signatures mirrored, which results in the image pull failing for cluster nodes at discovery. Specifically, the Agent image fails to connect with the cluster nodes, which causes communication with the assisted service to fail.

To work around this issue, apply an ignition override to the cluster that sets the `/etc/containers/policy.json` file to unrestrictive. The ignition override can be set in the `InfraEnv` custom resource definition. The following example shows an `InfraEnv` custom resource definition:

[source.yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: cluster
  namespace: cluster
spec:
  ignitionConfigOverride: '{"ignition":{"version":"3.2.0"},"storage":{"files":[{"path":"/etc/containers/policy.json","mode":420,"overwrite":true,"contents":{"source":"data:text/plain;charset=utf-8;base64,ewogICAgImRlZmF1bHQiOiBbCiAgIrhrthrthwogICAgICAgICAgICAidHlwZSI6ICJpbnNlY3VyZUFjY2VwdEFueXRoaW5nIgogICAgICAgIH0KICAgIF0sCiAgICAidHJhbnNwb3J0cyI6CiAgICAgICAgewogICAgICAgICAgICAiZG9ja2VyLWRhZW1vbiI6CiAgICAgICAgICAgICAgICB7CiAgICAgICAgICAgICAgICAgICAgIiI6IFt7InR5cGUiOiJpbnNlY3VyZUFjY2VwdEFueXRoaW5nIn1dCiAgICAgICAgICAgICAgICB9CiAgICAgICAgfQp9"}}]}}' 
----

The following example shows the unrestrictive file that is created:
----

{
    "default": [
        {
            "type": "insecureAcceptAnything"
        }
    ],
    "transports": {
        "docker-daemon": {
        "": [
        {
            "type": "insecureAcceptAnything"
        }
        ]
    }
    }
}
----
 
After this setting is changed, the clusters should 
 
[#hosted-control-plane-mce]
== Hosted control plane

[#console-hosted-pending-import]
=== Console displays hosted cluster as Pending import 
//2.7:25594

If the annotation and `ManagedCluster` name do not match, the console displays the cluster as `Pending import`. The cluster cannot be used by the {mce-short}. The same issue happens when there is no annotation and the `ManagedCluster` name does not match the `Infra-ID` value of the `HostedCluster` resource."

[#add-node-pool-duplicate-version]
=== Console might list the same version multiple times when adding a node pool to a hosted cluster 
//2.7:ACM-2664

When you use the console to add a new node pool to an existing hosted cluster, the same version of {ocp-short} might appear more than once in the list of options. You can select any instance in the list for the version that you want. 


