[#hosted-control-planes-manage-bm]
= Managing hosted control plane clusters on bare metal (Technology Preview)

You can use the {mce} console to create and manage a {ocp} hosted cluster. Hosted control planes are available as a Technology Preview on bare metal.

[#hosted-prerequisites-bm]
== Prerequisites

You must configure hosted control planes for bare metal before you can create hosted control plane clusters. See xref:../../clusters/hosted_control_planes/configure_hosted_bm.adoc#configuring-hosting-service-cluster-configure-bm[Configuring the hosting cluster on bare metal (Technology Preview)] for more information.

[#scaling-the-nodepool]
== Scaling the NodePool object for a hosted cluster

You add nodes to your hosted cluster by scaling the NodePool object.  

. Scale the NodePool object to two nodes:

+
----
oc -n ${CLUSTERS_NAMESPACE} scale nodepool ${NODEPOOL_NAME} --replicas 2
----

+
The Cluster API agent provider randomly picks two agents that are then assigned to the hosted cluster. Those agents go through different states and finally join the hosted cluster as {ocp-short} nodes. The agents pass through states in the following order:
+ 
* `binding`
* `discovering`
* `insufficient`
* `installing`
* `installing-in-progress`
* `added-to-existing-cluster`

+
//lahinson - June 2023 - Need to add a step here to explain what the following command does
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent
----

+
See the following example output:

+
----
NAME                                   CLUSTER         APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign   
d9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign   
da503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign

oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient
----

+
. After the agents reach the `added-to-existing-cluster` state, verify that you can see the {ocp-short} nodes by entering the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE     VERSION
ocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f
----

+
Cluster Operators start to reconcile by adding workloads to the nodes. 

. Enter the following command to verify that two machines were created when you scaled up the `NodePool` object:

+
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get machines
----

+
See the following example output:

+
----
NAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION
hypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.12z
hypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.12z
----

+
The `clusterversion` reconcile process eventually reaches a point where only Ingress and Console cluster operators are missing:

+
//lahinson - June 2023 - Is the following code output, or a command followed by output?
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.12z: the cluster operator console has not yet successfully rolled out

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get "https://console-openshift-console.apps.hypercluster1.domain.com": dial tcp 10.19.3.29:443: connect: connection refused
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      9m16s   
clusteroperator.config.openshift.io/image-registry                             4.12z    True        False         False      9m5s    
clusteroperator.config.openshift.io/ingress                                    4.12z    True        False         True       39m     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)
clusteroperator.config.openshift.io/insights                                   4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/kube-apiserver                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-controller-manager                    4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-scheduler                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/monitoring                                 4.12z    True        False         False      7m38s   
clusteroperator.config.openshift.io/network                                    4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/openshift-apiserver                        4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-controller-manager               4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-samples                          4.12z    True        False         False      8m54s   
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/service-ca                                 4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/storage                                    4.12z    True        False         False      11m 
----

[#handling-ingress]
== Handling Ingress in a hosted cluster on bare metal

Every {ocp-short} cluster comes set up with a default application ingress controller that is expected have an external DNS record associated with it. For example, if you create a HyperShift cluster named `example` with the base domain `krnl.es`, you can expect the wildcard domain
`*.apps.example.krnl.es` to be routable.

You can set up a load balancer and wildcard DNS record for the `*.apps`. This process requires deploying MetalLB, configuring a new load balancer service that routes to the ingress deployment, and assigning a wildcard DNS entry to the load balancer IP address.

. Set up MetalLB so that when you create a service of the `LoadBalancer` type, MetalLB adds an external IP address for the service.

+
.. Create a YAML file that contains the configuration for the MetalLB Operator:

+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: metallb
  labels:
    openshift.io/cluster-monitoring: "true"
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator-operatorgroup
  namespace: metallb
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator
  namespace: metallb
spec:
  channel: "stable"
  name: metallb-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----

.. Save the file as `metallb-operator-config.yaml`.

.. Enter the following command to apply the configuration:

+
----
oc apply -f metallb-operator-config.yaml
----

. After the Operator is running, create the MetalLB instance:


.. Create a YAML file that contains the configuration for the MetalLB instance:

+
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb
----

.. Save the file as `metallb-instance-config.yaml`.

.. Create the MetalLB instance by entering this command:

+
----
oc apply -f metallb-instance-config.yaml
----

. Configure the MetalLB Operator by creating two resources:

+
** An `IPAddressPool` resource with a single IP address. This IP address must be on the same subnet as the network that the cluster nodes use.
** A `BGPAdvertisement` resource to advertise the load balancer IP addresses that the `IPAddressPool` resource provides through the BGP protocol.
+
**Important:** Change the `INGRESS_IP` environment variable to match your environment's address.

+
.. Create a YAML file to contain the configuration:

+
[source,yaml]
----
export INGRESS_IP=192.168.122.23

apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-public-ip
  namespace: metallb
spec:
  protocol: layer2
  autoAssign: false
  addresses:
    - ${INGRESS_IP}-${INGRESS_IP}
---
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: ingress-public-ip
  namespace: metallb
spec:
  ipAddressPools:
    - ingress-public-ip
----

.. Save the file as `ipaddresspool-bgpadvertisement-config.yaml`.

.. Create the resources by entering the following command:

+
----
oc apply -f ipaddresspool-bgpadvertisement-config.yaml
----

. Expose the {ocp-short} Router via MetalLB by following these steps:

.. Create a YAML file to set up the LoadBalancer Service that routes ingress traffic to the ingress deployment:

+
[source,yaml]
----
kind: Service
apiVersion: v1
metadata:
  annotations:
    metallb.universe.tf/address-pool: ingress-public-ip
  name: metallb-ingress
  namespace: openshift-ingress
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443
  selector:
    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  type: LoadBalancer
----

.. Save the file as `metallb-loadbalancer-service.yaml`.

.. Enter the following command to apply the configuration from the YAML file:

+
----
oc apply -f metallb-loadbalancer-service.yaml
----

.. Enter the following command to reach the {ocp-short} console:

+
----
curl -kI https://console-openshift-console.apps.example.krnl.es

HTTP/1.1 200 OK
----

.. Check the `clusterversion` and `clusteroperator` values to verify that everything is running. Enter the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co
----

+
See the following example output:

+
----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.12z    True        False         3m32s   Cluster version is 4.12z

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    True        False         False      3m50s   
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/image-registry                             4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/ingress                                    4.12z    True        False         False      53m     
clusteroperator.config.openshift.io/insights                                   4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/kube-apiserver                             4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-controller-manager                    4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-scheduler                             4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/monitoring                                 4.12z    True        False         False      21m     
clusteroperator.config.openshift.io/network                                    4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/openshift-apiserver                        4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/openshift-controller-manager               4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/openshift-samples                          4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/service-ca                                 4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/storage                                    4.12z    True        False         False      25m     
----

For more information about MetalLB, see link:https://docs.openshift.com/container-platform/4.13/networking/metallb/about-metallb.html[About MetalLB and the MetalLB Operator] in the {ocp-short} documentation.

[#enable-node-auto-scaling-hosted-cluster]
== Enabling node auto-scaling for the hosted cluster

When you need more capacity in your hosted cluster and spare agents are available, you can enable auto-scaling to install new agents. 

. To enable auto-scaling, enter the following command. In this case, the minimum number of nodes is 2, and the maximum number is 5. The maximum number of nodes that you can add is bound by the number of available agents.

+
----
oc -n ${CLUSTERS_NAMESPACE} patch nodepool ${HOSTED_CLUSTER_NAME} --type=json -p '[{"op": "remove", "path": "/spec/replicas"},{"op":"add", "path": "/spec/autoScaling", "value": { "max": 5, "min": 2 }}]'
----

+
If 10 minutes pass without requiring the additional capacity, the agent is decommissioned and placed in the spare queue again.

. Create a workload that requires a new node.

.. Create a YAML file that contains the workload configuration, as shown in the following example:

+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: reversewords
  name: reversewords
  namespace: default
spec:
  replicas: 40
  selector:
    matchLabels:
      app: reversewords
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: reversewords
  spec:
    containers:
    - image: quay.io/mavazque/reversewords:latest
      name: reversewords
      resources:
        requests:
          memory: 2Gi
status: {}
----

.. Save the file as `workload-config.yaml`.

.. Apply the YAML by entering the following command:

+
----
oc apply -f workload-config.yaml 
----

. Verify that the remaining agents are deployed by entering the following command. In this example, the spare agent, `d9198891-39f4-4930-a679-65fb142b108b`, is provisioned:

+
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'
----

+
See the following example output:

+
----
BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: installing-in-progress
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster
----

. If you check the nodes by entering the following command, the new node is displayed in the output. In this example, `ocp-worker-0` is added to the cluster:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE   VERSION
ocp-worker-0   Ready    worker   35s   v1.24.0+3882f8f
ocp-worker-1   Ready    worker   40m   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   41m   v1.24.0+3882f8f
----

. To remove the node, delete the workload by entering the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig -n default delete deployment reversewords
----

. Wait 10 minutes and then confirm that the node was removed by entering the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE   VERSION
ocp-worker-1   Ready    worker   51m   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   52m   v1.24.0+3882f8f
----

+
//lahinson - June 2023 - Should the following command be preceded by a step? Or is it part of the example output?
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: added-to-existing-cluster
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: added-to-existing-cluster
----

[#hypershift-cluster-destroy-bm]
== Destroying a hosted cluster on bare metal

You can use the console to destroy bare metal hosted clusters. Complete the following steps to destroy a hosted cluster on bare metal:

. In the console, navigate to *Infrastructure* > *Clusters*.

. On the _Clusters_ page, select the cluster that you want to destroy.

. In the *Actions* menu, select *Destroy clusters* to remove the cluster.

[#additional-resources-manage-bm]
== Additional resources

* You can now deploy the SR-IOV Operator. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes] to learn more about deploying the SR-IOV Operator.

