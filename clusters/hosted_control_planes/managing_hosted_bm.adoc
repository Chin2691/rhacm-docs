[#hosted-control-planes-manage-bm]
= Managing hosted control plane clusters on bare metal

You can use the {mce} console to create and manage a {ocp} hosted cluster. Hosted control planes are available as a Technology Preview on bare metal.

[#hosted-prerequisites-bm]
== Prerequisites

You must configure hosted control planes for bare metal before you can create hosted control plane clusters. See xref:../../clusters/hosted_control_planes/configure_hosted_bm.adoc#configuring-hosting-service-cluster-configure-bm[Configuring the hosting cluster on bare metal] for more information.

[#scaling-the-nodepool]
== Scaling the NodePool object for a hosted cluster

You add nodes to your hosted cluster by scaling the NodePool object.  

. Scale the NodePool object to two nodes:

+
----
oc -n ${CLUSTERS_NAMESPACE} scale nodepool ${NODEPOOL_NAME} --replicas 2
----

+
The Cluster API agent provider randomly picks two agents that are then assigned to the hosted cluster. Those agents go through different states and finally join the hosted cluster as {ocp-short} nodes. The agents pass through states in the following order:

+ 
* `binding`
* `discovering`
* `insufficient`
* `installing`
* `installing-in-progress`
* `added-to-existing-cluster`

+
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent
----
//lahinson - June 2023 - Need to add a step above to explain what this command does

+
See the following example output:

+
----
NAME                                   CLUSTER         APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign   
d9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign   
da503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign

oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'

BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient
----

+
. After the agents reach the `added-to-existing-cluster` state, verify that you can see the {ocp-short} nodes by entering the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE     VERSION
ocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f
----

+
Cluster Operators start to reconcile by adding workloads to the nodes. 

. Enter the following command to verify that two machines were created when you scaled up the `NodePool` object:

+
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get machines
----

+
See the following example output:

+
----
NAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION
hypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.12z
hypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.12z
----

+
The `clusterversion` reconcile process eventually reaches a point where only Ingress and Console cluster operators are missing:
//lahinson - June 2023 - Is the following code output, or a command followed by output?

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co

NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.12z: the cluster operator console has not yet successfully rolled out

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get "https://console-openshift-console.apps.hypercluster1.domain.com": dial tcp 10.19.3.29:443: connect: connection refused
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      9m16s   
clusteroperator.config.openshift.io/image-registry                             4.12z    True        False         False      9m5s    
clusteroperator.config.openshift.io/ingress                                    4.12z    True        False         True       39m     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)
clusteroperator.config.openshift.io/insights                                   4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/kube-apiserver                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-controller-manager                    4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-scheduler                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/monitoring                                 4.12z    True        False         False      7m38s   
clusteroperator.config.openshift.io/network                                    4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/openshift-apiserver                        4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-controller-manager               4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-samples                          4.12z    True        False         False      8m54s   
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/service-ca                                 4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/storage                                    4.12z    True        False         False      11m 
----

[#handling-ingress]
== Handling Ingress in a hosted cluster on bare metal

Every {ocp-short} cluster comes set up with a default application ingress controller that is expected have an external DNS record associated with it. For example, if you create a HyperShift cluster named `example` with the base domain `krnl.es`, you can expect the wildcard domain
`*.apps.example.krnl.es` to be routable.

You can set up a load balancer and wildcard DNS record for the `*.apps`. This process requires deploying MetalLB, configuring a new load balancer service that routes to the ingress deployment, and assigning a wildcard DNS entry to the load balancer IP address.

. Set up MetalLB so that when you create a service of the `LoadBalancer` type, MetalLB adds an external IP address for the service.

+
.. Create a YAML file that contains the configuration for the MetalLB Operator:

+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: metallb
  labels:
    openshift.io/cluster-monitoring: "true"
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator-operatorgroup
  namespace: metallb
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator
  namespace: metallb
spec:
  channel: "stable"
  name: metallb-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----

.. Save the file as `metallb-operator-config.yaml`.

.. Enter the following command to apply the configuration:

+
----
oc apply -f metallb-operator-config.yaml
----

. After the Operator is running, create the MetalLB instance:


.. Create a YAML file that contains the configuration for the MetalLB instance:

+
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb
----

.. Save the file as `metallb-instance-config.yaml`.

.. Create the MetalLB instance by entering this command:

+
----
oc apply -f metallb-instance-config.yaml
----

. Configure the MetalLB Operator by creating two resources:

+
** An `IPAddressPool` resource with a single IP address. This IP address must be on the same subnet as the network that the cluster nodes use.
** A `BGPAdvertisement` resource to advertise the load balancer IP addresses that the `IPAddressPool` resource provides through the BGP protocol.
+
**Important:** Change the `INGRESS_IP` environment variable to match your environment's address.

+
.. Create a YAML file to contain the configuration:

+
[source,yaml]
----
export INGRESS_IP=192.168.122.23

apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-public-ip
  namespace: metallb
spec:
  protocol: layer2
  autoAssign: false
  addresses:
    - ${INGRESS_IP}-${INGRESS_IP}
---
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: ingress-public-ip
  namespace: metallb
spec:
  ipAddressPools:
    - ingress-public-ip
----

+
.. Save the file as `ipaddresspool-bgpadvertisement-config.yaml`.

+
.. Create the resources by entering the following command:

+
----
oc apply -f ipaddresspool-bgpadvertisement-config.yaml
----

. Expose the {ocp-short} Router via MetalLB by following these steps:

.. Create a YAML file to set up the LoadBalancer Service that routes ingress traffic to the ingress deployment:

+
[source,yaml]
----
kind: Service
apiVersion: v1
metadata:
  annotations:
    metallb.universe.tf/address-pool: ingress-public-ip
  name: metallb-ingress
  namespace: openshift-ingress
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443
  selector:
    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  type: LoadBalancer
----

.. Save the file as `metallb-loadbalancer-service.yaml`.

.. Enter the following command to apply the configuration from the YAML file:

+
----
oc apply -f metallb-loadbalancer-service.yaml
----

.. Enter the following command to reach the {ocp-short} console:

+
----
curl -kI https://console-openshift-console.apps.example.krnl.es

HTTP/1.1 200 OK
----

.. Check the `clusterversion` and `clusteroperator` values to verify that everything is running. Enter the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get clusterversion,co
----

+
See the following example output:

+
----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.12z    True        False         3m32s   Cluster version is 4.12z

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    True        False         False      3m50s   
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/image-registry                             4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/ingress                                    4.12z    True        False         False      53m     
clusteroperator.config.openshift.io/insights                                   4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/kube-apiserver                             4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-controller-manager                    4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-scheduler                             4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/monitoring                                 4.12z    True        False         False      21m     
clusteroperator.config.openshift.io/network                                    4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/openshift-apiserver                        4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/openshift-controller-manager               4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/openshift-samples                          4.12z    True        False         False      23m     
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.12z    True        False         False      54m     
clusteroperator.config.openshift.io/service-ca                                 4.12z    True        False         False      25m     
clusteroperator.config.openshift.io/storage                                    4.12z    True        False         False      25m     
----

For more information about MetalLB, see link:https://docs.openshift.com/container-platform/4.13/networking/metallb/about-metallb.html[About MetalLB and the MetalLB Operator] in the {ocp-short} documentation.

[#hypershift-cluster-destroy-bm]
== Destroying a hosted cluster on bare metal

You can use the console to destroy bare metal hosted clusters. Complete the following steps to destroy a hosted cluster on bare metal:

. In the console, navigate to *Infrastructure* > *Clusters*.

. On the _Clusters_ page, select the cluster that you want to destroy.

. In the *Actions* menu, select *Destroy clusters* to remove the cluster.

[#additional-resources-manage-bm]
== Additional resources

* To configure node auto-scaling, see xref:../hosted_control_planes/node-autoscaling_hosted_cluster.adoc#enable-node-auto-scaling-hosted-cluster[Enabling node auto-scaling for the hosted cluster]

* You can now deploy the SR-IOV Operator. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes] to learn more about deploying the SR-IOV Operator.

