[#volsync]
= Replicating persistent volumes with VolSync (Technology Preview)

VolSync is a Kubernetes operator that enables asynchronous replication of persistent volumes within a cluster, or across clusters with storage types that are not otherwise compatible for replication. It uses the Container Storage Interface (CSI) to overcome the compatibility limitation. After deploying the VolSync operator in your environment, you can leverage it to create and maintain copies of your persistent data.

**Note:** VolSync does not meet the requirements of the FIPS standard. 

There are three methods that you can use to replicate when you use VolSync, which depend on the number of synchronization locations that you have. The Rsync method is used for this example. For information about the other methods and more information about Rsync, see https://volsync.readthedocs.io/en/latest/usage/index.html[Usage] in the VolSync documentation.  

Rsync replication is a one-to-one replication of persistent volumes, and is likely to be the most commonly used. This is used for replicating data to a remote site. 

[#volsync-prereq]
== Prerequisites

Before installing VolSync on your clusters, you must have the following requirements:

* A configured {ocp} environment running a {product-title-short} version 2.4, or later, hub cluster.

* At least two configured clusters that are managed by the same {product-title-short} hub cluster.

* The storage driver that you use for your source persistent volume must be CSI-compatible and able to support snapshots. 

[#volsync-install-clusters]
== Installing VolSync on the managed clusters

To enable VolSync on two clusters in your environment, you must install it on both the source and the target managed clusters. Complete the following steps to leverage the policy template in {product-title-short} to install VolSync:

. Download the https://github.com/stolostron/policy-collection/blob/main/community/CM-Configuration-Management/policy-persistent-data-management.yaml[policy-persistent-data-management] policy template from the GitHub repository. In its current format, this policy starts an installation of VolSync on the hub cluster. After it is installed on the hub cluster, it automatically installs the required VolSync components on all of the managed clusters of the hub.
+
*Note:* If you downloaded and used this policy before {product-title-short} version 2.4.2, download the file again. The path has changed for the file and some of its references.  
+
You can specify a different label to identify which clusters will have VolSync installed by modifying the following section of the policy file:
+
[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      vendor: OpenShift
----
+
*Note:* Do not modify the content for the `clusterSelector` later in the file that applies to the `local-cluster`.
 
. Apply the policy while you are logged in to your hub cluster by entering the following command:
+
----
oc -n <namespace> apply -f ./policy-persistent-data-management.yaml
----
+
Replace `namespace` with a namespace on your hub cluster.

[#volsync-rsync-clusters]
== Configuring Rsync replication across managed clusters

For Rsync-based replication, configure custom resources on the source and destination clusters. The custom resources use the `address` value to connect the source to the destination, and the `sshKeys` to ensure that the transferred data is secure.

**Note:** You must copy the values for `address` and `sshKeys` from the destination to the source, so configure the destination before you configure the source.

This example provides the steps to configure an Rsync replication from a persistent volume claim on the `source` cluster in the `source-ns` namespace to a persistent volume claim on a `destination` cluster in the `destination-ns` namespace. You can replace those values with other values, if necessary.

. Configure your destination cluster.

.. Run the following command on the destination cluster to create the namespace:
+
----
$ kubectl create ns <destination-ns>
----
+
Replace `destination-ns` with a name for the namespace that will contain your destination persistent volume claim.

.. Copy the following YAML content to create a new file called `replication_destination.yaml`:
+
[source,yaml]
----
---
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: <destination>
  namespace: <destination-ns>
spec:
  rsync:
    serviceType: LoadBalancer
    copyMethod: Snapshot
    capacity: 2Gi
    accessModes: [ReadWriteOnce]
    storageClassName: gp2-csi
    volumeSnapshotClassName: csi-aws-vsc
----
+
*Note:* The `capacity` value should match the capacity of the persistent volume claim that is being replicated.
+
Replace `destination` with the name of your replication destination CR.
+
Replace `destination-ns` with the name of the namespace where your destination is located.
+
For this example, the `ServiceType` value of `LoadBalancer` is used. The load balancer service is created by the source cluster to enable your source managed cluster to transfer information to a different destination managed cluster. You can use `ClusterIP` as the service type if your source and destinations are on the same cluster. Note the address of the Load Balancer and the name of the secret to refer to when you configure the source cluster.
+ 
The `storageClassName` and `volumeSnapshotClassName` are optional parameters. Specify the values for your environment, particularly if you are using a storage class and volume snapshot class name that are different than the default values for your environment. 

.. Run the following command on the destination cluster to create the `replicationdestination` resource:
+
----
$ kubectl create -n <destination-ns> -f replication_destination.yaml
----
+
Replace `destination-ns` with the name of the namespace where your destination is located.
+
After the `replicationdestination` resource is created, following parameters and values are added to the resource: 
+
|==========
| Parameter | Value

| `.status.rsync.address` | IP address of the destination cluster that is used to enable the source and destination clusters to communicate.
| `.status.rsync.sshKeys` | Name of the SSH key file that enables secure data transfer from the source cluster to the destination cluster. 
|==========

.. Run the following command to copy the value of `.status.rsync.address` to use on the source cluster:
+
----
$ ADDRESS=`kubectl get replicationdestination <destination> -n <destination-ns> --template={{.status.rsync.address}}`
$ echo $ADDRESS
----
+
Replace `destination` with the name of your replication destination CR.
+
Replace `destination-ns` with the name of the namespace where your destination is located.
+
The output should appear similar to the following output, which is for an Amazon Web Services environment:
+
----
a831264645yhrjrjyer6f9e4a02eb2-5592c0b3d94dd376.elb.us-east-1.amazonaws.com
----

.. Run the following command to copy the name of the secret and the contents of the secret that are provided as the value of `.status.rsync.sshKeys`.
+
----
$ SSHKEYS=`kubectl get replicationdestination <destination> -n <destination-ns> --template={{.status.rsync.sshKeys}}`
$ echo $SSHKEYS
----
+
Replace `destination` with the name of your replication destination CR.
+
Replace `destination-ns` with the name of the namespace where your destination is located.
+
You will have to enter it on the source cluster when you configure the source. The output should be the name of your SSH keys secret file, which might resemble the following name:
+
----
volsync-rsync-dst-src-destination-name
----

. Identify the source persistent volume claim that you want to replicate.
+
*Note:* The source persistent volume claim must be on a CSI storage class.

. Create the `ReplicationSource` items.
+
.. Copy the following YAML content to create a new file called `replication_source.yaml` on the source cluster: 
+
[source,yaml]
----
---
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: <source>
  namespace: <source-ns>
spec:
  sourcePVC: <persistent_volume_claim>
  trigger:
    schedule: "*/3 * * * *"
  rsync:
    sshKeys: <mysshkeys>
    address: <my.host.com>
    copyMethod: Snapshot
    storageClassName: gp2-csi
    volumeSnapshotClassName: csi-aws-vsc
----
+
Replace `source` with the name for your replication source CR. See step _3-vi_ in this procedure for instructions on how to replace this automatically.
+
Replace `source-ns` with the namespace of the persistent volume claim where your source is located. See step _3-vi_ in this procedure for instructions on how to replace this automatically. 
+
Replace `persistent_volume_claim` with the name of your source persistent volume claim.
+
Replace `mysshkeys` with the keys that you copied from the `.status.rsync.sshKeys` field of the `ReplicationDestination` when you configured it. 
+
Replace `my.host.com` with the host address that you copied from the `.status.rsync.address` field of the `ReplicationDestination` when you configured it. 
+
If your storage driver supports cloning, using `Clone` as the value for `copyMethod` might be a more streamlined process for the replication.
+ 
`StorageClassName` and `volumeSnapshotClassName` are optional parameters. If you are using a storage class and volume snapshot class name that are different than the defaults for your environment, specify those values. 
+
You can now set up the synchronization method of the persistent volume.

.. Copy the SSH secret from the destination cluster by entering the following command against the destination cluster:
+
----
$ kubectl get secret -n <destination-ns> $SSHKEYS -o yaml > /tmp/secret.yaml
----
+
Replace `destination-ns` with the namespace of the persistent volume claim where your destination is located.

.. Open the secret file in the `vi` editor by entering the following command:
+
----
$ vi /tmp/secret.yaml
----

.. In the open secret file on the destination cluster, make the following changes:
+
* Change the namespace to the namespace of your source cluster. For this example, it is `source-ns`.
* Remove the owner references (`.metadata.ownerReferences`).

.. On the source cluster, create the secret file by entering the following command on the source cluster:
+
----
$ kubectl create -f /tmp/secret.yaml
----

.. On the source cluster, modify the `replication_source.yaml` file by replacing the value of the `address` and `sshKeys` in the `ReplicationSource` object with the values that you noted from the destination cluster by entering the following commands:
+
----
$ sed -i "s/<my.host.com>/$ADDRESS/g" replication_source.yaml
$ sed -i "s/<mysshkeys>/$SSHKEYS/g" replication_source.yaml
$ kubectl create -n <source> -f replication_source.yaml
----
+
Replace `my.host.com` with the host address that you copied from the `.status.rsync.address` field of the `ReplicationDestination` when you configured it.
+
Replace `mysshkeys` with the keys that you copied from the `.status.rsync.sshKeys` field of the `ReplicationDestination` when you configured it.
+
Replace `source` with the name of the persistent volume claim where your source is located.
+
*Note:* You must create the the file in the same namespace as the persistent volume claim that you want to replicate. 

.. Verify that the replication completed by running the following command on the `ReplicationSource` object:
+
----
$ kubectl describe ReplicationSource -n <source-ns> <source>
----
+
Replace `source-ns` with the namespace of the persistent volume claim where your source is located.
+
Replace `source` with the name of your replication source CR. 
+
If the replication was successful, the output should be similar to the following example:
+
----
Status:
  Conditions:
    Last Transition Time:  2021-10-14T20:48:00Z
    Message:               Synchronization in-progress
    Reason:                SyncInProgress
    Status:                True
    Type:                  Synchronizing
    Last Transition Time:  2021-10-14T20:41:41Z
    Message:               Reconcile complete
    Reason:                ReconcileComplete
    Status:                True
    Type:                  Reconciled
  Last Sync Duration:      5m20.764642395s
  Last Sync Time:          2021-10-14T20:47:01Z
  Next Sync Time:          2021-10-14T20:48:00Z
----
+
If the `Last Sync Time` has no time listed, then the replication is not complete.

[#volsync-convert-backup-pvc]
== Converting a replicated image to a usable persistent volume claim

There might be a time when you need to use the replicated image to recover data or to create a new instance of a persistent volume claim. The copy of the image must be converted to a persistent volume claim before it can be used. To convert a replicated image to a persistent volume claim, complete the following steps:

. When the replication is complete, identify the latest snapshot from the `ReplicationDestination` object by entering the following command:
+
----
$ kubectl get replicationdestination <destination> -n <destination-ns> --template={{.status.latestImage.name}}
----
Note the value of the latest snapshot for when you create your persistent volume claim.
+
Replace `destination` with the name of your replication destination. 
+
Replace `destination-ns` with the namespace of your destination. 

. Create a `pvc.yaml` file that resembles the following example:
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: <pvc-name>
  namespace: <destination-ns>
spec:
  accessModes:
    - ReadWriteOnce
  dataSource:
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
    name: <snapshot_to_replace>
  resources:
    requests:
      storage: 2Gi
----
+
Replace `pvc-name` with a name for your new persistent volume claim.
+
Replace `destination-ns` with the namespace where the persistent volume claim is located.  
+
Replace `snapshot_to_replace` with the `VolumeSnapshot` name that you found in the previous step.
+
You can update `resources.requests.storage` with a different value, but it is best practice when the value is at least the same size as the initial source persistent volume claim.

. Validate that your persistent volume claim is running in the environment by entering the following command:
+
----
$ kubectl get pvc -n <destination-ns>
----

[#volsync-start]
== Scheduling your synchronization

You have a few options to select from when determining how you start your replications: always running, on a schedule, or manually. Scheduling your replications is an option that is the option that is often selected. 

The *Schedule* option runs replications at scheduled times. A schedule is defined by a `cronspec`, so the schedule can be configured as intervals of time or as specific times. The order of the schedule values are:

`"minute (0-59) hour (0-23) day-of-month (1-31) month (1-12) day-of-week (0-6)"`

The replication starts when the scheduled time occurs. Your setting for this replication option might resemble the following content:

[source,yaml]
----
spec:
  trigger:
    schedule: "*/6 * * * *"
----

After enabling one of these methods, your synchronization schedule runs according to the method that you configured.

See the https://volsync.readthedocs.io/en/latest/index.html[VolSync] documentation for additional information and options.
