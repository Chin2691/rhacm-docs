[#volsync]
= Replicating persistent volumes with VolSync (Technology Preview)

VolSync is a Kubernetes operator that enables asynchronous replication of persistent volumes within a cluster, or across clusters with storage types that are not otherwise compatible for replication. It uses the Container Storage Interface (CSI) to overcome the compatibility limitation. After deploying the VolSync operator in your environment, you can leverage it to create and maintain copies of your persistent data.

**Note:** VolSync does not meet the requirements of the FIPS standard. 

There are three methods that you can use to replicate when you use VolSync, which depend on the number of synchronization locations that you have. The Rsync method is used for this example. For information about the other methods and more information about Rsync, see https://volsync.readthedocs.io/en/latest/usage/index.html[Usage] in the VolSync documentation.  

Rsync replication is a one-to-one replication of persistent volumes, and is likely to be the most commonly used. This is used for replicating data to a remote site. 

[#volsync-prereq]
== Prerequisites

Before installing VolSync on your clusters, you must have the following requirements:

* A configured {ocp} environment running a {product-title-short} version 2.4, or later, hub cluster.

* At least two configured clusters that are managed by the same {product-title-short} hub cluster.

* The storage driver that you use for your source persistent volume must be CSI-compatible and able to support snapshots. 

[#volsync-install-clusters]
== Installing VolSync on the managed clusters

To enable VolSync on two clusters in your environment, you must install it on both the source and the target managed clusters. Complete the following steps to leverage the policy template in {product-title-short} to install VolSync:

. Download the https://github.com/stolostron/policy-collection/blob/main/community/CM-Configuration-Management/policy-persistent-data-management.yaml[policy-persistent-data-management] policy template from the GitHub repository. In its current format, this policy starts an installation of VolSync on the hub cluster. After it is installed on the hub cluster, it automatically installs the required VolSync components on all of the managed clusters of the hub.
+
*Note:* If you downloaded and used this policy before {product-title-short} version 2.4.2, download the file again. The path has changed for the file and some of its references.  
+
You can specify a different label to identify which clusters will have VolSync installed by modifying the following section of the policy file:
+
[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      vendor: OpenShift
----
+
*Note:* Do not modify the content for the `clusterSelector` later in the file that applies to the `local-cluster`.
 
. Apply the policy while you are logged in to your hub cluster by entering the following command:
+
----
oc -n <namespace> apply -f ./policy-persistent-data-management.yaml
----
+
Replace `namespace` with a namespace on your hub cluster.

[#volsync-rsync-clusters]
== Configiring Rsync replication across managed clusters

For Rsync-based replication, configure custom resources on the source and destination clusters. The custom resources use the `address` value to connect the source to the destination, and the `sshKeys` to ensure that the transferred data is secure.
+
**Note:** You must copy the values for `address` and `sshKeys` from the destination to the source, so configure the destination before you configure the source.

This example provides the steps to configure an Rsync replication from a PVC on the `source` cluster in the `source` namespace to a PVC on a `destination` cluster in the `dest` namespace.

. Configure your destination cluster named `destination`.

.. Run the following command on the destination cluster to create the namespace:
+
----
$ kubectl create ns dest
----

.. Copy the following YAML content to create a new file on the destination cluster:
+
[source,yaml]
----
---
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: <destination_name>
  namespace: <dest>
spec:
  rsync:
    serviceType: LoadBalancer
    copyMethod: Snapshot
    capacity: 2Gi
    accessModes: [ReadWriteOnce]
    storageClassName: gp2-csi
    volumeSnapshotClassName: gp2-csi
----
+
*Note:* The `capacity` value should match the capacity of the persistent volume claim that is being replicated.
+
Replace `destination_name` with the name of your destination volume.
+
Replace `dest` with the name of the namespace where your destination is located.
+
For this example, the `ServiceType` value of `LoadBalancer` is used. The load balancer service is created by the source cluster to enable your source managed cluster to transfer information to a different destination managed cluster. You can use `ClusterIP` as the service type if your source and destinations are on the same cluster. Note the address of the Load Balancer and the name of the secret to refer to when you configure the source cluster.
+ 
The `storageClassName` and `volumeSnapshotClassName` are optional parameters. If you are using a storage class and volume snapshot class name that are different than the defaults for your environment, specify those values. 

.. Run the following command on the destination cluster to create the `replicationdestination` resource:
+
----
$ kubectl create -n dest -f examples/rsync/volsync_v1alpha1_replicationdestination_remotecluster.yaml
----

.. Run the following command to copy the value of `.status.rsync.address`:
+
----
$ ADDRESS=`kubectl get replicationdestination database-destination -n dest --template={{.status.rsync.address}}`
$ echo $ADDRESS
----
+
This value is automatically generated when the custom resource is created. You will have to enter it when you configure the source. The output should appear similar to the following output, which is for an Amazon Web Services environment:
+
----
a831264645yhrjrjyer6f9e4a02eb2-5592c0b3d94dd376.elb.us-east-1.amazonaws.com
----

.. Run the following command to copy the name of the secret and the contents of the secret that are provided as the value of `.status.rsync.sshKeys`.
+
----
$ SSHKEYS=`kubectl get replicationdestination database-destination -n dest  --template={{.status.rsync.sshKeys}}`
$ echo $SSHKEYS
----
+
You will have to enter it on the source cluster when you configure the source. The output should be the name of your SSH keys secret file, which might resemble the following name:
+
----
volsync-rsync-dst-src-database-destination
----

. Deploy the database. For this example, a database is copied. You might use an application. 

... On the source cluster, `source-cluster`, enter the following commands to create and prepare the database: 
+
----
$ kubectl create ns source
$ kubectl create -n source -f examples/source-database
----

.. Run the following command to confirm that the database is running:
+
----
$ kubectl get pods -n source
----
+
The returned information should resemble the following example:
+
----
NAME                    READY   STATUS    RESTARTS   AGE
mysql-8b9c5c8d8-24w6g   1/1     Running   0          17s
----

. Create the `ReplicationSource` items.
+
Your replication source file should look similar to the following example, when you are complete: 
+
[source,yaml]
----
---
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: <database-source>
  namespace: <source>
spec:
  sourcePVC: <persistent_volume_claim>
  trigger:
    schedule: "*/3 * * * *"
  rsync:
    sshKeys: <volsync-rsync-destination-src-database-destination>
    address: <source.host.com>
    copyMethod: Snapshot
    storageClassName: gp2-csi
    volumeSnapshotClassName: gp2-csi
----
+
Replace `database_source` with a unique name for your replication.
+
Replace `source` with the name of the namespace where your source is located.
+
Replace `persistent_volume_claim` with the name of your source claim.
+
Replace `volsync-rsync-destination-src-database-destination` with the keys that you copied from the `.status.rsync.sshKeys` field of the `ReplicationDestination` when you configured it. 
+
Replace `source.host.com` with the host address that you copied from the `.status.rsync.address` field of the `ReplicationDestination` when you configured it. 
+
If your storage driver supports cloning, using `Clone` as the value for `copyMethod` might be a more streamlined process for the replication.
+ 
The `storageClassName` and `volumeSnapshotClassName` are optional parameters. If you are using a storage class and volume snapshot class name that are different than the defaults for your environment, specify those values. 

You can now set up the synchronization method of the persistent volume.

.. Copy the SSH secret from the destination cluster by entering the following command on the destination cluster:
+
----
$ kubectl get secret -n dest $SSHKEYS -o yaml > /tmp/secret.yaml
----

.. Open the secret file in the `vi` editor by entering the following command:
+
----
$ vi /tmp/secret.yaml
----

.. In the open secret file on the destination cluster, make the following changes:
+
* Change the namespace to the namespace of your source cluster. For this example, it is `source`.
* Remove the owner references (`.metadata.ownerReferences`).

.. On the source cluster, create the secret file by entering the following command on the source cluster:
+
----
$ kubectl create -f /tmp/secret.yaml
----
+
*Note:* In this example, the source and the destination PVs are on the same server. If your PVs are not on the same server, then copy your secret file to the server of your source PV. 

.. On the source cluster, modify the `volsync_v1alpha1_replicationsource_remotecluster.yaml` file by replacing the value of the `address` and `sshKeys` in the `ReplicationSource` object with the values that you noted from the destintation cluster by entering the following commands:
+
----
$ sed -i "s/my.host.com/$ADDRESS/g" examples/rsync/volsync_v1alpha1_replicationsource_remotecluster.yaml
$ sed -i "s/mysshkeys/$SSHKEYS/g" examples/rsync/volsync_v1alpha1_replicationsource_remotecluster.yaml
$ kubectl create -n source -f examples/rsync/volsync_v1alpha1_replicationsource_remotecluster.yaml
----

.. Verify that the replication completed by running the following command on the `ReplicationSource` object:
+
----
$ kubectl describe ReplicationSource -n source database-source
----
+
If the replication was successful, the output should be similar to the following example:
+
----
Status:
  Conditions:
    Last Transition Time:  2021-10-14T20:48:00Z
    Message:               Synchronization in-progress
    Reason:                SyncInProgress
    Status:                True
    Type:                  Synchronizing
    Last Transition Time:  2021-10-14T20:41:41Z
    Message:               Reconcile complete
    Reason:                ReconcileComplete
    Status:                True
    Type:                  Reconciled
  Last Sync Duration:      5m20.764642395s
  Last Sync Time:          2021-10-14T20:47:01Z
  Next Sync Time:          2021-10-14T20:48:00Z
----
+
If the `Last Sync Time` has no time listed, then the replication is not complete. 

.. Create a database in the `mysql` pod that is running in the source namespace by entering the following commands on the destination cluster:
+
----
$ kubectl exec --stdin --tty -n source `kubectl get pods -n source | grep mysql | awk '{print $1}'` -- /bin/bash
$ mysql -u root -p$MYSQL_ROOT_PASSWORD
> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.00 sec)


> create database synced;
> exit
$ exit
----
+
The `mysql` database is deployed to the `dest` namespace, which uses the replicated data. 

... Identify the latest snapshot from the `ReplicationDestination` object by entering the following command:
+
----
$ kubectl get replicationdestination database-destination -n dest --template={{.status.latestImage.name}}
----
Note the value of the latest snapshot for when you create your PVC. 

.. Create the Deployment, Service, PVC, and Secret by entering the following commands on the destination cluster:
+
----
$ sed -i 's/snapshotToReplace/volsync-dest-database-destination-20201203174504/g' examples/destination-database/mysql-pvc.yaml
$ kubectl create -n dest -f examples/destination-database/
----

.. Validate that the `mysql` pod is running in the environment by entering the following command:
+
----
$ kubectl get pods -n dest
----

.. Connect to the `mysql` pod and list the databases to verify that the synchronized database exists by entering the following command:
+
----
$ kubectl exec --stdin --tty -n dest `kubectl get pods -n dest | grep mysql | awk '{print $1}'` -- /bin/bash
$ mysql -u root -p$MYSQL_ROOT_PASSWORD
> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| synced             |
| sys                |
+--------------------+
5 rows in set (0.00 sec)
----

[#volsync-start]
== Scheduling your synchronization

You have a few options to select from when determining how you start your replications: always running, on a schedule, or manually. Scheduling your replications is an option that is the option that is often selected. 

The *Schedule* option runs replications at scheduled times. A schedule is defined by a `cronspec`, so the schedule can be configured as intervals of time or as specific times. The order of the schedule values are:

`"minute (0-59) hour (0-23) day-of-month (1-31) month (1-12) day-of-week (0-6)"`

The replication starts when the scheduled time occurs. Your setting for this replication option might resemble the following content:

[source,yaml]
----
spec:
  trigger:
    schedule: "*/6 * * * *"
----

After enabling one of these methods, your synchronization schedule runs according to the method that you configured.

See the https://volsync.readthedocs.io/en/latest/index.html[VolSync] documentation for additional information and options.
