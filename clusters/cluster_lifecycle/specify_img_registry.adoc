[#specify-registry-img-on-managed-clusters-for-import]
= Specifying image registry on managed clusters for import

You might need to override the image registry on the managed clusters that you are importing. You can do this by creating a `ManagedClusterImageRegistry` custom resource definition. 

The `ManagedClusterImageRegistry` custom resource definition is a namespace-scoped resource.

The `ManagedClusterImageRegistry` custom resource definition specifies a set of managed clusters for a Placement to select, but needs different images from the custom image registry. After the managed clusters are updated with the new images, the following label is added to each managed cluster for identification: `open-cluster-management.io/image-registry=<namespace>.<managedClusterImageRegistryName>`.

The following example shows a `ManagedClusterImageRegistry` custom resource definition:

[source,yaml]
----
apiVersion: imageregistry.open-cluster-management.io/v1alpha1
kind: ManagedClusterImageRegistry
metadata:
  name: <imageRegistryName>
  namespace: <namespace>
spec:
  placementRef:
    group: cluster.open-cluster-management.io
    resource: placements
    name: <placementName> <1>
  pullSecret:
    name: <pullSecretName> <2>
  registries: <3>
  - mirror: <mirrored-image-registry-address>
    source: <image-registry-address>
  - mirror: <mirrored-image-registry-address>
    source: <image-registry-address>
----
<1> Replace with the name of a Placement in the same namespace that selects a set of managed clusters.
<2> Replace with the name of the pull secret that is used to pull images from the custom image registry.
<3> List the values for each of the `source` and `mirror` registries. Replace the `mirrored-image-registry-address` and `image-registry-address` with the value for each of the `mirror` and `source` values of the registries. 

** Example 1: To replace the source image registry named `registry.redhat.io/rhacm2` with `localhost:5000/rhacm2`, and `registry.redhat.io/multicluster-engine` with `localhost:5000/multicluster-engine`, use the following example:

[source,yaml]
----
registries:
- mirror: localhost:5000/rhacm2/
    source: registry.redhat.io/rhacm2
- mirror: localhost:5000/multicluster-engine
    source: registry.redhat.io/multicluster-engine
----

** Example 2: To replace the source image, `registry.redhat.io/rhacm2/registration-rhel8-operator` with `localhost:5000/rhacm2-registration-rhel8-operator`, use the following example:

+
[source,yaml]
----
registries:
- mirror: localhost:5000/rhacm2-registration-rhel8-operator
    source: registry.redhat.io/rhacm2/registration-rhel8-operator
----

*Important:* If you are importing a managed cluster by using agent registration, you must create a `KlusterletConfig` that contains image registries. See the following example. Replace values where needed:

[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: <klusterletconfigName>
spec:
  pullSecret:
    namespace: <pullSecretNamespace>
    name: <pullSecretName>
  registries:
    - mirror: <mirrored-image-registry-address>
      source: <image-registry-address>
    - mirror: <mirrored-image-registry-address>
      source: <image-registry-address>
----
 
See xref:../cluster_lifecycle/import_agent.adoc#importing-managed-agent[Importing a managed cluster by using the agent registration endpoint] to learn more.

[#import-cluster-managedclusterimageregistry]
== Importing a cluster that has a _ManagedClusterImageRegistry_

Complete the following steps to import a cluster that is customized with a ManagedClusterImageRegistry custom resource definition: 

. Create a pull secret in the namespace where you want your cluster to be imported. For these steps, the namespace is `myNamespace`.
+
----
$ kubectl create secret docker-registry myPullSecret \
  --docker-server=<your-registry-server> \
  --docker-username=<my-name> \
  --docker-password=<my-password>
----

. Create a Placement in the namespace that you created.
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: myPlacement
  namespace: myNamespace
spec:
  clusterSets:
  - myClusterSet
  tolerations:
  - key: "cluster.open-cluster-management.io/unreachable"
    operator: Exists
----
+
*Note:* The `unreachable` toleration is required for the Placement to be able to select the cluster. 

. Create a `ManagedClusterSet` resource and bind it to your namespace.
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSet
metadata:
  name: myClusterSet
  
---
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSetBinding
metadata:
  name: myClusterSet
  namespace: myNamespace
spec:
  clusterSet: myClusterSet
----

. Create the `ManagedClusterImageRegistry` custom resource definition in your namespace.
+
[source,yaml]
----
apiVersion: imageregistry.open-cluster-management.io/v1alpha1
kind: ManagedClusterImageRegistry
metadata:
  name: myImageRegistry
  namespace: myNamespace
spec:
  placementRef:
    group: cluster.open-cluster-management.io
    resource: placements
    name: myPlacement
  pullSecret:
    name: myPullSecret
  registry: myRegistryAddress
----

. Import a managed cluster from the console and add it to a managed cluster set.

. Copy and run the import commands on the managed cluster after the label `open-cluster-management.io/image-registry=myNamespace.myImageRegistry` is added to the managed cluster.

[#configure-proxy-settings-for-observability-add-ons]
== Configuring proxy settings for Observability add-ons

Configure the proxy settings to allow the communications from the managed cluster to access the hub cluster through a HTTP and HTTPS proxy server. Typically, add-ons do not need any special configuration to support HTTP and HTTPS proxy servers between a hub cluster and a managed cluster. But if you enabled the Observability add-on, you must complete the following configuration: 

.Prerequisite 
- You have a hub cluster. 
- You have enabled the proxy settings between the hub cluster and managed cluster. 


. Go to the cluster namespace on your hub cluster. 
. Create an `AddOnDeploymentConfig` with the proxy settings. For an example, see the following yaml:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: AddOnDeploymentConfig
metadata:
  name: <addon-deploy-config-name>
  namespace: <managed-cluster-name>
spec:
  agentInstallNamespace: open-cluster-managment-addon-observability
  proxyConfig:
    httpsProxy: "https://<username>:<password>@<ip>:<port>"
    noProxy: ".cluster.local,.svc,172.30.0.1"
----

.. Specify the same name for the HTTP and HTTPS proxy. For example, `http://<username>:<password>@<ip>:<port>`. 
.. For the field, `noProxy`, include the IP address of the `kube-apiserver`. 
.. To access this IP address, run the following command on your managed cluster: `oc -n default describe svc kubernetes | grep IP:`. 
. Go to the `ManagedClusterAddOn`.
. Reference the `AddOnDeploymentConfig` you made by adding an item in `spec.configs`. For an example, see the following yaml:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: AddOnDeploymentConfig
metadata:
  name: <addon-deploy-config-name>
  namespace: <managed-cluster-name>
spec:
  agentInstallNamespace: open-cluster-managment-addon-observability
  proxyConfig:
    httpsProxy: "https://<username>:<password>@<ip>:<port>"
    noProxy: ".cluster.local,.svc,172.30.0.1"
----

[#disabling-proxy-settings-for-observability-add-ons]
== Disabling proxy settings for Observability add-ons

If your development needs change, you might need to disable the Observability add-ons you configured for the hub cluster and managed cluster. You can disable the Observability add-on at any time. ,

. Go to the `ManagedClusterAddOn`.
. Remove the referenced `AddOnDeploymentConfig`. 
